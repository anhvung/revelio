{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# < 1h\n",
    "# Make basic version with no nlp\n",
    "\n",
    "# Clean code, rearrange in clear steps : panda join, slipt, preprocessing, training\n",
    "# clean Variable names (make sure it still works), add comments\n",
    "\n",
    "\n",
    "# ~1h\n",
    "# make the diy nlp with most occuring degree terms\n",
    "# test if imporvements\n",
    "\n",
    "\n",
    "\n",
    "# pandas output\n",
    "#email to ask \n",
    "\n",
    "# test rnn\n",
    "\n",
    "# improvements: cross check with friend list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b4ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785a570",
   "metadata": {},
   "source": [
    "# 0. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f90003",
   "metadata": {},
   "source": [
    "Downloading the data, so that we don't have to do it again if we restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a5f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data downloaded\n"
     ]
    }
   ],
   "source": [
    "urls=[\"https://info0.s3.us-east-2.amazonaws.com/recruitment/positions.csv\",\"https://info0.s3.us-east-2.amazonaws.com/recruitment/education.csv\",\"https://info0.s3.us-east-2.amazonaws.com/recruitment/jobtitle_seniority.csv\"]\n",
    "\n",
    "# data folder, create if it does not exists\n",
    "os.makedirs('./data/', exist_ok=True) \n",
    "    \n",
    "# if .csv file does not exists, download it\n",
    "for url in urls:\n",
    "    path='./data/' + os.path.basename(url)\n",
    "    if not os.path.exists(path):\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "        \n",
    "print('data downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c1a1d",
   "metadata": {},
   "source": [
    "Loading the data into pandas data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d89c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO dl?\n",
    "#TODO S3 bucket code?\n",
    "df_positions = pd.read_csv(\"./data/positions.csv\")\n",
    "df_education = pd.read_csv(\"./data/education.csv\")\n",
    "df_seniority = pd.read_csv(\"./data/jobtitle_seniority.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0837c4",
   "metadata": {},
   "source": [
    "# 1. Checking the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Individual data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36a0326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>jobtitle</th>\n",
       "      <th>startdate</th>\n",
       "      <th>enddate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162425</th>\n",
       "      <td>zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>student_senior_service_college</td>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>2013-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15089</th>\n",
       "      <td>zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>command_general_staff_college</td>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2005-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146169</th>\n",
       "      <td>zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>engineer_company_brigade_staff_trainer</td>\n",
       "      <td>2002-04-01</td>\n",
       "      <td>2004-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15923</th>\n",
       "      <td>zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>student_engineer_officer</td>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>1998-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20486</th>\n",
       "      <td>zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>company_commander_battalion_battalion_assistant</td>\n",
       "      <td>1998-11-01</td>\n",
       "      <td>2002-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309939</th>\n",
       "      <td>zzrNxfUzwZXNkSs15haLyA4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>partner_head_private_client_department_|_law_p...</td>\n",
       "      <td>1992-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338504</th>\n",
       "      <td>zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...</td>\n",
       "      <td>coordinador_de_personal_embarcado</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334912</th>\n",
       "      <td>zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...</td>\n",
       "      <td>operador_|_logistics_supply_chain</td>\n",
       "      <td>2014-11-01</td>\n",
       "      <td>2015-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359048</th>\n",
       "      <td>zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...</td>\n",
       "      <td>supervisor_de_personal_|_maritime</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>2017-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248375</th>\n",
       "      <td>zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...</td>\n",
       "      <td>supervisor_de_personal_|_logistics_supply_chain</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>2016-05-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  user_id  \\\n",
       "162425  zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "15089   zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "146169  zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "15923   zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "20486   zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "309939  zzrNxfUzwZXNkSs15haLyA4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "338504  zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...   \n",
       "334912  zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...   \n",
       "359048  zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...   \n",
       "248375  zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...   \n",
       "\n",
       "                                                 jobtitle   startdate  \\\n",
       "162425                     student_senior_service_college  2012-08-01   \n",
       "15089                       command_general_staff_college  2004-07-01   \n",
       "146169             engineer_company_brigade_staff_trainer  2002-04-01   \n",
       "15923                            student_engineer_officer  1998-01-01   \n",
       "20486     company_commander_battalion_battalion_assistant  1998-11-01   \n",
       "309939  partner_head_private_client_department_|_law_p...  1992-01-01   \n",
       "338504                  coordinador_de_personal_embarcado  2017-12-01   \n",
       "334912                  operador_|_logistics_supply_chain  2014-11-01   \n",
       "359048                  supervisor_de_personal_|_maritime  2016-05-01   \n",
       "248375    supervisor_de_personal_|_logistics_supply_chain  2015-11-01   \n",
       "\n",
       "           enddate  \n",
       "162425  2013-06-01  \n",
       "15089   2005-06-01  \n",
       "146169  2004-06-01  \n",
       "15923   1998-10-01  \n",
       "20486   2002-03-01  \n",
       "309939         NaN  \n",
       "338504         NaN  \n",
       "334912  2015-09-01  \n",
       "359048  2017-09-01  \n",
       "248375  2016-05-01  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positions=df_positions.sort_values(by=['user_id'])\n",
    "df_positions.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca9cec",
   "metadata": {},
   "source": [
    "#### Looking at the dates, linkedIn only has the year and month information, so date formatting is YYYY-MM-DD\n",
    "#### Position titles and fields are seperated by '\\_|\\_', words are seperated by '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e299739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>major</th>\n",
       "      <th>startdate</th>\n",
       "      <th>enddate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99508</th>\n",
       "      <td>++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>BS</td>\n",
       "      <td>1949-01-01</td>\n",
       "      <td>1953-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92083</th>\n",
       "      <td>++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>BS in Electronics</td>\n",
       "      <td>1973-01-01</td>\n",
       "      <td>1978-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92505</th>\n",
       "      <td>++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>1987-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133238</th>\n",
       "      <td>++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>Master Grande Ecole</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5126</th>\n",
       "      <td>++6zEVtPCi83vpPTHSY2Vg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>Bachelor of Science (B.Sc.) (ED)</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2006-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  user_id  \\\n",
       "99508   ++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "92083   ++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "92505   ++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "133238  ++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...   \n",
       "5126    ++6zEVtPCi83vpPTHSY2Vg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "\n",
       "                                   major   startdate     enddate  \n",
       "99508                                 BS  1949-01-01  1953-01-01  \n",
       "92083                  BS in Electronics  1973-01-01  1978-01-01  \n",
       "92505                                NaN  1984-01-01  1987-01-01  \n",
       "133238               Master Grande Ecole  2013-01-01  2016-01-01  \n",
       "5126    Bachelor of Science (B.Sc.) (ED)  2001-01-01  2006-01-01  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_education=df_education.sort_values(by=['user_id'])\n",
    "df_education.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c69fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>jobtitle</th>\n",
       "      <th>seniority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90141</th>\n",
       "      <td>++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>former_owner_presently_consultant</td>\n",
       "      <td>7.064817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71129</th>\n",
       "      <td>++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>design_engineer_|_mechanical_industrial_engine...</td>\n",
       "      <td>3.331507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222292</th>\n",
       "      <td>++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>owner_|_computer_network_security</td>\n",
       "      <td>7.334247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399946</th>\n",
       "      <td>++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>digital_communication_social_medias_activation...</td>\n",
       "      <td>4.307247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220871</th>\n",
       "      <td>++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>event_promoter_public_relations</td>\n",
       "      <td>1.908356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  user_id  \\\n",
       "90141   ++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "71129   ++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "222292  ++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "399946  ++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...   \n",
       "220871  ++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...   \n",
       "\n",
       "                                                 jobtitle  seniority  \n",
       "90141                   former_owner_presently_consultant   7.064817  \n",
       "71129   design_engineer_|_mechanical_industrial_engine...   3.331507  \n",
       "222292                  owner_|_computer_network_security   7.334247  \n",
       "399946  digital_communication_social_medias_activation...   4.307247  \n",
       "220871                    event_promoter_public_relations   1.908356  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seniority=df_seniority.sort_values(by=['user_id'])\n",
    "df_seniority.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e8fc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 226184 entries, 99508 to 169422\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   user_id    226184 non-null  object\n",
      " 1   major      162346 non-null  object\n",
      " 2   startdate  197556 non-null  object\n",
      " 3   enddate    190658 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 8.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 377585 entries, 41525 to 248375\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   user_id    377585 non-null  object\n",
      " 1   jobtitle   376136 non-null  object\n",
      " 2   startdate  368526 non-null  object\n",
      " 3   enddate    270354 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 14.4+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 416295 entries, 90141 to 126315\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    416295 non-null  object \n",
      " 1   jobtitle   414290 non-null  object \n",
      " 2   seniority  416295 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 12.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_education.info())\n",
    "print(df_positions.info())\n",
    "print(df_seniority.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the number of DISTINCT users\n",
    "def count_users(df):\n",
    "    return len(pd.unique(df['user_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6637ed",
   "metadata": {},
   "source": [
    "#### As expected a person can have multiple education, and position entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "781fde87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   major   startdate     enddate\n",
      "92083  BS in Electronics  1973-01-01  1978-01-01\n",
      "92505                NaN  1984-01-01  1987-01-01\n",
      "                                                 jobtitle   startdate  \\\n",
      "9781                    owner_|_computer_network_security  1993-06-01   \n",
      "106525  design_engineer_|_mechanical_industrial_engine...  1984-10-01   \n",
      "\n",
      "           enddate  \n",
      "9781           NaN  \n",
      "106525  1989-05-01  \n",
      "                                                 jobtitle  seniority\n",
      "71129   design_engineer_|_mechanical_industrial_engine...   3.331507\n",
      "222292                  owner_|_computer_network_security   7.334247\n"
     ]
    }
   ],
   "source": [
    "test_id =df_education.iat[2,0]\n",
    "def get_user_history(user_id):\n",
    "    print(df_education[df_education['user_id'].str.contains(user_id,regex=False,na=False)][['major', 'startdate','enddate']])\n",
    "    print(df_positions[df_positions['user_id'].str.contains(user_id,regex=False,na=False)][['jobtitle', 'startdate','enddate']])\n",
    "    print(df_seniority[df_seniority['user_id'].str.contains(user_id,regex=False,na=False)][['jobtitle', 'seniority']])\n",
    "get_user_history(test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values for each df\n",
    "There are more missing values for end dates because it can the current position / education program of someone\n",
    "\n",
    "Missing values for dates are more important than for education/jobtittles to predict the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf4b1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id           0\n",
      "jobtitle       1449\n",
      "startdate      9059\n",
      "enddate      107231\n",
      "dtype: int64\n",
      "------\n",
      "user_id          0\n",
      "major        63838\n",
      "startdate    28628\n",
      "enddate      35526\n",
      "dtype: int64\n",
      "------\n",
      "user_id         0\n",
      "jobtitle     2005\n",
      "seniority       0\n",
      "dtype: int64\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "#Missing values\n",
    "print(df_positions.isna().sum(),'------',sep='\\n')\n",
    "print(df_education.isna().sum(),'------',sep='\\n')\n",
    "print(df_seniority.isna().sum(),'------',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing starting dates AND enddates are problematic because it makes it hard to impute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6518 unique users missing job date info in df_positions\n",
      "0 unique users missing seniority info\n",
      "18036 unique users missing education date info\n"
     ]
    }
   ],
   "source": [
    "df_missing_pos=df_positions[df_positions['enddate'].isna() & df_positions['startdate'].isna()]\n",
    "df_missing_sen=df_seniority[df_seniority['seniority'].isna() ]\n",
    "df_missing_edu=df_education[df_education['enddate'].isna() & df_education['startdate'].isna()]\n",
    "print(count_users(df_missing_pos),'unique users missing job date info in df_positions')\n",
    "print(count_users(df_missing_sen),'unique users missing seniority info')\n",
    "print(count_users(df_missing_edu),'unique users missing education date info')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is even more true if there are no dates for both the education and job entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981 unique users missing dates on all education and job info\n"
     ]
    }
   ],
   "source": [
    "df_missing_all=df_missing_pos.merge(df_missing_edu, how='inner',left_on=['user_id'], right_on=['user_id'])\n",
    "print(count_users(df_missing_all),'unique users missing dates on all education and job info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Splitting the data to get training data \n",
    "\n",
    "## dataprocessing will be done after to prevent data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Choosing the training Data\n",
    "\n",
    "People with highscool information have ages that are easy to predict since most people end highschool at the same age.\n",
    "The same is true for Bachelors. \n",
    "As a general rule, the ealier the education is in terms of degree, the better it is to predict the age because of the smaller variability.\n",
    "\n",
    "Because there are more people with bachelor information (1/3 of the data) we will use the starting date of the bachelor to determine someone's age for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3368 unique users with a highschool info\n"
     ]
    }
   ],
   "source": [
    "mask_high = df_education.major.apply(lambda x: ('highschool' in str(x).lower()) or ('high school' in str(x).lower()) )\n",
    "print(count_users(df_education[mask_high]),'unique users with a highschool info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37463 unique users with a bachelor info\n"
     ]
    }
   ],
   "source": [
    "bachelors=['bachelor','bs ','ba ','b.s.','b.a.']\n",
    "mask_ba = df_education.major.apply(lambda x:  any(bachelor in str(x).lower() for bachelor in bachelors))\n",
    "print(count_users(df_education[mask_ba]),'unique users with a bachelor info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33931 unique users with a bachelor info containing exactly 'bachelor'\n"
     ]
    }
   ],
   "source": [
    "bachelors=['bachelor']\n",
    "mask_ba = df_education.major.apply(lambda x:  any(bachelor in str(x).lower() for bachelor in bachelors))\n",
    "print(count_users(df_education[mask_ba]),'unique users with a bachelor info containing exactly \\'bachelor\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the later as it is simpler to work with, and 1/3 of the data as training data is enough.\n",
    "##### In the more advanced versions, we use NLP and clustering to encode similer majors, taking into account abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_users=pd.unique(df_education[mask_ba]['user_id']).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Splitting the data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_df(users,df):\n",
    "\n",
    "    users_train,users_val=train_test_split( users, test_size = 0.3, random_state = 42)\n",
    "    df_train=df.loc[df['user_id'].isin(users_train)]\n",
    "    df_val=df.loc[df['user_id'].isin(users_val)]\n",
    "    df_test=df.loc[~ df['user_id'].isin(users)]\n",
    "    return df_train,df_val,df_test\n",
    "\n",
    "df_positions_train,df_positions_val,df_positions_test=split_df(training_users,df_positions)\n",
    "df_education_train,df_education_val,df_education_test=split_df(training_users,df_education)\n",
    "df_seniority_train,df_seniority_val,df_seniority_test=split_df(training_users,df_seniority)\n",
    "\n",
    "\n",
    "# checking that we still have every user\n",
    "assert(count_users(df_positions_train)+count_users(df_positions_val)+count_users(df_positions_test)==count_users(df_positions)) #posisions does not have 100k users\n",
    "assert(count_users(df_education_train)+count_users(df_education_val)+count_users(df_education_test)==count_users(df_education))\n",
    "assert(count_users(df_seniority_train)+count_users(df_seniority_val)+count_users(df_seniority_test)==count_users(df_seniority))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0:\n",
      " of\n",
      " bachelor\n",
      " bachelor of\n",
      " arts\n",
      " of arts\n",
      " bachelor of arts\n",
      " ba\n",
      " engineering\n",
      " bachelor of arts ba\n",
      " of arts ba\n",
      "-------\n",
      "class 1:\n",
      " degree\n",
      " bachelor\n",
      " bachelor degree\n",
      " in\n",
      " degree in\n",
      " bachelor degree in\n",
      " of\n",
      " degree of\n",
      " bachelor degree of\n",
      " engineering\n",
      "-------\n",
      "class 2:\n",
      " diploma\n",
      " masters\n",
      " degree\n",
      " in\n",
      " certificate\n",
      " bachelors\n",
      " school\n",
      " of\n",
      " high\n",
      " high school\n",
      "-------\n",
      "class 3:\n",
      " master\n",
      " degree\n",
      " master degree\n",
      " in\n",
      " master degree in\n",
      " degree in\n",
      " master 39 degree\n",
      " 39 degree\n",
      " 39\n",
      " master 39\n",
      "-------\n",
      "class 4:\n",
      " science\n",
      " of\n",
      " of applied\n",
      " applied science\n",
      " applied\n",
      " of applied science\n",
      " bachelor of applied science\n",
      " bachelor of applied\n",
      " bachelor\n",
      " bachelor of\n",
      "-------\n",
      "class 5:\n",
      " of\n",
      " business\n",
      " of business\n",
      " administration\n",
      " of business administration\n",
      " business administration\n",
      " bachelor\n",
      " bachelor of\n",
      " bachelor of business\n",
      " bachelor of business administration\n",
      "-------\n",
      "class 6:\n",
      " of\n",
      " master\n",
      " master of\n",
      " administration\n",
      " business\n",
      " master of business\n",
      " of business\n",
      " master of business administration\n",
      " of business administration\n",
      " business administration\n",
      "-------\n",
      "class 7:\n",
      " science\n",
      " of\n",
      " of science\n",
      " master of science\n",
      " master of\n",
      " master\n",
      " master of science ms\n",
      " science ms\n",
      " ms\n",
      " of science ms\n",
      "-------\n",
      "class 8:\n",
      " of\n",
      " science\n",
      " bachelor\n",
      " bachelor of\n",
      " of science\n",
      " bachelor of science\n",
      " bs\n",
      " bachelor of science bs\n",
      " of science bs\n",
      " science bs\n",
      "-------\n",
      "class 9:\n",
      " of philosophy\n",
      " of\n",
      " philosophy\n",
      " doctor\n",
      " doctor of philosophy\n",
      " doctor of\n",
      " philosophy phd\n",
      " phd\n",
      " of philosophy phd\n",
      " doctor of philosophy phd\n",
      "-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\AppData\\Local\\Temp/ipykernel_13508/2790559756.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['major_cluster']=major_clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0:\n",
      " mining\n",
      " metals\n",
      " mining metals\n",
      " engineer\n",
      " manager\n",
      " engineer mining\n",
      " engineer mining metals\n",
      " intern\n",
      " senior\n",
      " intern mining\n",
      "-------\n",
      "class 1:\n",
      " services\n",
      " technology\n",
      " information\n",
      " information technology\n",
      " technology services\n",
      " information technology services\n",
      " manager\n",
      " engineer\n",
      " engineer information technology\n",
      " engineer information\n",
      "-------\n",
      "class 2:\n",
      " engineer\n",
      " education\n",
      " intern\n",
      " management\n",
      " senior\n",
      " services\n",
      " analyst\n",
      " sales\n",
      " higher\n",
      " higher education\n",
      "-------\n",
      "class 3:\n",
      " assistant\n",
      " education\n",
      " higher\n",
      " higher education\n",
      " manager\n",
      " research\n",
      " assistant higher education\n",
      " assistant higher\n",
      " research assistant\n",
      " administrative\n",
      "-------\n",
      "class 4:\n",
      " fitness\n",
      " health\n",
      " wellness\n",
      " health wellness\n",
      " wellness fitness\n",
      " health wellness fitness\n",
      " manager\n",
      " manager health\n",
      " manager health wellness\n",
      " manager health wellness fitness\n",
      "-------\n",
      "class 5:\n",
      " director\n",
      " marketing\n",
      " advertising\n",
      " marketing advertising\n",
      " services\n",
      " education\n",
      " executive\n",
      " sales\n",
      " management\n",
      " development\n",
      "-------\n",
      "class 6:\n",
      " administration\n",
      " government\n",
      " government administration\n",
      " assistant\n",
      " manager\n",
      " officer\n",
      " intern\n",
      " officer government administration\n",
      " officer government\n",
      " director\n",
      "-------\n",
      "class 7:\n",
      " manager\n",
      " project\n",
      " project manager\n",
      " sales\n",
      " account\n",
      " marketing\n",
      " senior\n",
      " sales manager\n",
      " account manager\n",
      " services\n",
      "-------\n",
      "class 8:\n",
      " media\n",
      " broadcast\n",
      " broadcast media\n",
      " social\n",
      " social media\n",
      " production\n",
      " media production\n",
      " marketing\n",
      " manager\n",
      " intern\n",
      "-------\n",
      "class 9:\n",
      " technology\n",
      " analyst\n",
      " information\n",
      " services\n",
      " information technology\n",
      " information technology services\n",
      " technology services\n",
      " analyst information technology\n",
      " analyst information\n",
      " analyst information technology services\n",
      "-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\AppData\\Local\\Temp/ipykernel_13508/2790559756.py:216: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['job_cluster']=jobs_clusters\n",
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\core\\indexing.py:1951: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[selected_item_labels] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing dates and text format \n",
      "\n",
      "completing position end dates...\n",
      "added 24017 values \n",
      "\n",
      "completing education end dates...\n",
      "added 1815 values \n",
      "\n",
      "completing education start dates...\n",
      "average length:  3.608725981355846 yrs\n",
      "added 398 values \n",
      "\n",
      "completing position end dates...\n",
      "average length:  3.962133030775858 yrs\n",
      "added 21 values \n",
      "\n",
      "completing education remaining dates...\n",
      "average start date:  2003.2742277481523 yrs\n",
      "average end date:  2006.882953729532 yrs\n",
      "added 6235 values \n",
      "\n",
      "completing position remaining dates...\n",
      "average start date:  2010.1799402438633 yrs\n",
      "average end date:  2014.1420732741742 yrs\n",
      "added 1635 values \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\core\\generic.py:6392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n",
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return(text.split('_'))\n",
    "\n",
    "class DataTransformer:\n",
    "    # to preven data leakage\n",
    "    #class constants, used for test data too, but they are only changed with the training data when fit=true\n",
    "    \n",
    "    avg_length_job= avg_length_edu = avg_end_job = avg_start_job= avg_end_edu = avg_start_edu =0\n",
    "    majors_pipe = jobs_pipe = None\n",
    "    \n",
    "    def __init__(self,df_positions,df_education,df_seniority):\n",
    "        self.df_positions = df_positions\n",
    "        self.df_education=df_education\n",
    "        self.df_seniority = df_seniority\n",
    "    ### \n",
    "    def format_to_size(self,df,labels=False):\n",
    "        def to_fixed_size(df,cols,size):\n",
    "            for col in cols:\n",
    "                df[col] = df[col].apply(lambda x: x[0:size] if (len(x)>size) else  (x + (size-len(x)) * [x[-1]]))\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        return to_fixed_size(df,['enddate_y','startdate_y','seniority','enddate_x','startdate_x','major_cluster','job_cluster'],10)\n",
    "    ###\n",
    "    def get_labels(self,df,remove_info=False):\n",
    "        df['label']=None\n",
    "        def get_first_index(mylist, substring):\n",
    "            for i, s in enumerate(mylist):\n",
    "                if substring in s:\n",
    "                      return i\n",
    "            return -1\n",
    "        for index, row in df.iterrows():\n",
    "            i=get_first_index(row['major'],'bachelor')\n",
    "            birth=row['startdate_y'][i]-18\n",
    "            if i!=-1:\n",
    "                df.loc[index,'label'] = birth \n",
    "                if remove_info and len(row['enddate_y'])>1 and random.random()>0.3: # remove bachelor info for training\n",
    "                    df.at[index,'enddate_y'] = row['enddate_y'][:i]+row['enddate_y'][i+1:]\n",
    "                    df.at[index,'startdate_y'] = row['startdate_y'][:i]+row['startdate_y'][i+1:]\n",
    "                    df.at[index,'major'] =  row['major'][:i]+row['major'][i+1:]\n",
    "            else:\n",
    "                print('ERROR COULD NOT GET LABEL FOR USER:', row.index.name)\n",
    "        return df\n",
    "    ###\n",
    "    def merge_pd(self,df_positions, df_education,df_seniority,labels=False,remove_info=False):\n",
    "        # sorting by startdate for later:\n",
    "        df_positions.sort_values(by=['startdate'],inplace=True)\n",
    "        df_education.sort_values(by=['startdate'],inplace=True)\n",
    "        \n",
    "        df=df_positions.merge(df_seniority, how='right',left_on=['user_id','jobtitle'], right_on=['user_id','jobtitle'])\n",
    "        df=df.groupby('user_id').agg(lambda x: list(x))\n",
    "        df_education_temp=df_education.groupby('user_id').agg(lambda x: list(x))\n",
    "        df=df.merge(df_education_temp, how='outer', left_on='user_id', right_on='user_id')\n",
    "        \n",
    "        df['startdate_x']=df.apply(lambda row: [max(row['enddate_y']) if pd.isnull(date) else date for date in row['startdate_x'] ] ,axis=1)\n",
    "        now = datetime.now()\n",
    "        today=now.year+now.month/13\n",
    "        df['enddate_x']=df['enddate_x'].apply(lambda dates: [today if pd.isnull(date) else date for date in dates])\n",
    "        \n",
    "        if labels: # add labels\n",
    "            df=self.get_labels(df,remove_info)\n",
    "        return df\n",
    "    ###\n",
    "    def transform_pd(self,df_positions, df_education,df_seniority,fit=False):\n",
    "        \n",
    "        def to_lower_case(df,col):\n",
    "            df.loc[:,col] =  df[col].str.lower()\n",
    "            return df \n",
    "        def to_datetime(df,col):\n",
    "            df.loc[:,col] =  pd.to_datetime(df[col], format='%Y-%m-%d')\n",
    "            df.loc[:,col] =  df[col].dt.year+ df[col].dt.month/13\n",
    "            return df\n",
    "        print('changing dates and text format \\n')\n",
    "\n",
    "        df_education = df_education.pipe(to_lower_case, col='major').pipe(to_datetime,col='startdate').pipe(to_datetime,col='enddate')\n",
    "        df_positions = df_positions.pipe(to_lower_case, col='jobtitle').pipe(to_datetime,col='startdate').pipe(to_datetime,col='enddate')\n",
    "        df_seniority = df_seniority.pipe(to_lower_case, col='jobtitle')\n",
    "        \n",
    "        def fill_end_dates(df):\n",
    "            now = datetime.now()\n",
    "            today=now.year+now.month/13\n",
    "            #print('today',today)\n",
    "            before=df['enddate'].isna().sum()\n",
    "            df.loc[:,'enddate']=df.loc[:,:].apply(lambda row: today if pd.isnull(row['enddate']) and (not pd.isnull(row['startdate'])) else row['enddate'],axis=1)\n",
    "            print('added',before-df['enddate'].isna().sum(),'values \\n')\n",
    "            return df\n",
    "\n",
    "        print('completing position end dates...')\n",
    "\n",
    "        df_positions=fill_end_dates(df_positions)\n",
    "        print('completing education end dates...')\n",
    "        df_education=fill_end_dates(df_education)\n",
    "\n",
    "        def fill_startdate(df,typ):\n",
    "            df_nona=df[['startdate','enddate']].dropna(how='all',inplace=False)\n",
    "            df_nona['length']=df_nona['enddate']-df_nona['startdate']\n",
    "            avg_length=0\n",
    "            if fit:\n",
    "                if typ=='edu':\n",
    "                    DataTransformer.avg_length_edu=avg_length=df_nona[\"length\"].mean()\n",
    "                else:\n",
    "                    DataTransformer.avg_length_job=avg_length=df_nona[\"length\"].mean()\n",
    "            else:\n",
    "                if typ=='edu':\n",
    "                    avg_length=DataTransformer.avg_length_edu\n",
    "                else:\n",
    "                    avg_length= DataTransformer.avg_length_job\n",
    "                \n",
    "            print('average length: ',avg_length,'yrs')\n",
    "            before=df['startdate'].isna().sum()\n",
    "            df['startdate'].fillna(df['enddate']-avg_length, inplace=True)\n",
    "            print('added',before-df['startdate'].isna().sum(),'values \\n')\n",
    "            return df\n",
    "        print('completing education start dates...')\n",
    "        df_education=fill_startdate(df_education,'edu')\n",
    "        print('completing position end dates...')\n",
    "        df_positions=fill_startdate(df_positions,'job')\n",
    "        \n",
    "        def fill_dates_missingall(df,typ):\n",
    "            df_nona=df[['startdate','enddate']].dropna(how='all',inplace=False)\n",
    "            #df_nona['length']=df_nona['enddate']-df_nona['startdate']\n",
    "            \n",
    "            avg_start=avg_end=0\n",
    "            if fit:\n",
    "                if typ=='edu':\n",
    "                    DataTransformer.avg_start_edu=avg_start=df_nona[\"startdate\"].mean()\n",
    "                    DataTransformer.avg_end_edu=avg_end=df_nona[\"enddate\"].mean()\n",
    "                else:\n",
    "                    DataTransformer.avg_start_job=avg_start=df_nona[\"startdate\"].mean()\n",
    "                    DataTransformer.avg_end_job=avg_end=df_nona[\"enddate\"].mean()\n",
    "            else:\n",
    "                if typ=='edu':\n",
    "                    avg_start=DataTransformer.avg_start_edu\n",
    "                    avg_end=DataTransformer.avg_end_edu\n",
    "                else:\n",
    "                    avg_start=DataTransformer.avg_start_job\n",
    "                    avg_end=DataTransformer.avg_end_job\n",
    "            \n",
    "                \n",
    "            print('average start date: ',avg_start,'yrs')\n",
    "            print('average end date: ',avg_end,'yrs')\n",
    "            before=df['startdate'].isna().sum()\n",
    "            df['startdate'].fillna(avg_start, inplace=True)\n",
    "            df['enddate'].fillna(avg_end, inplace=True)\n",
    "            print('added',before-df['startdate'].isna().sum(),'values \\n')\n",
    "            return df  \n",
    "        print('completing education remaining dates...')\n",
    "        df_education=fill_dates_missingall(df_education,'edu')\n",
    "        print('completing position remaining dates...')\n",
    "        df_positions=fill_dates_missingall(df_positions,'job')\n",
    "        \n",
    "        # filling missing education and positions\n",
    "        df_positions['jobtitle'].fillna('positions', inplace=True)\n",
    "        df_seniority['jobtitle'].fillna('positions', inplace=True)\n",
    "        df_education['major'].fillna('education', inplace=True)\n",
    "        \n",
    "        #filling missing seniority information\n",
    "        df_seniority['seniority'].fillna((df_seniority['seniority'].mean()), inplace=True)\n",
    "            \n",
    "        \n",
    "        return df_positions,df_education,df_seniority\n",
    "    \n",
    "    ####\n",
    "    def get_edu_clusters(self,df,fit,remove_info):\n",
    "        if fit:\n",
    "            processed_text=[]\n",
    "            for t in df['major'].tolist():\n",
    "                r=0\n",
    "                if remove_info:\n",
    "                    r=random.random() # if training or val data remove bachelor data to match the testing data bachelor ratio\n",
    "                if (not pd.isnull(t)) and (not 'bachelor' in t.lower() or r <0.33): # removing one third of bechelors to match the rest of the data set\n",
    "                    processed_text.append(t)\n",
    "            # clustering pipeline   \n",
    "\n",
    "            number_of_clusters = 10\n",
    "            DataTransformer.majors_pipe = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 4))), ('majors_model', KMeans(n_clusters=number_of_clusters, init='k-means++', max_iter=50, n_init=2))])\n",
    "            DataTransformer.majors_pipe.fit(processed_text)\n",
    "            order_centroids = DataTransformer.majors_pipe['majors_model'].cluster_centers_.argsort()[:, ::-1]\n",
    "            terms = DataTransformer.majors_pipe['vectorizer'].get_feature_names_out()\n",
    "            for i in range(0,number_of_clusters):\n",
    "                print(\"class %d:\" % i),\n",
    "                for ind in order_centroids[i, :10]:\n",
    "                    print(' %s' % terms[ind])\n",
    "                print('-------') \n",
    "                \n",
    "        # add cluster info to df\n",
    "        majors=df['major'].astype(str).tolist()\n",
    "        major_clusters=DataTransformer.majors_pipe.predict(majors)\n",
    "        df['major_cluster']=major_clusters\n",
    "        \n",
    "        return df\n",
    "    ###\n",
    "    def get_position_clusters(self,df,fit):\n",
    "        if fit:\n",
    "            processed_text=[]\n",
    "            for t in df['jobtitle'].tolist():\n",
    "           \n",
    "                if (not pd.isnull(t)): # removing one third of bechelors to match the rest of the data set\n",
    "                    processed_text.append(t)\n",
    "            # clustering pipeline   \n",
    "\n",
    "            number_of_clusters = 10\n",
    "            DataTransformer.jobs_pipe = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 4),stop_words=['|'])), ('jobs_model', KMeans(n_clusters=number_of_clusters, init='k-means++', max_iter=50, n_init=2))])\n",
    "            DataTransformer.jobs_pipe.fit(processed_text)\n",
    "            order_centroids = DataTransformer.jobs_pipe['jobs_model'].cluster_centers_.argsort()[:, ::-1]\n",
    "            terms = DataTransformer.jobs_pipe['vectorizer'].get_feature_names_out()\n",
    "            for i in range(0,number_of_clusters):\n",
    "                print(\"class %d:\" % i),\n",
    "                for ind in order_centroids[i, :10]:\n",
    "                    print(' %s' % terms[ind])\n",
    "                print('-------') \n",
    "                \n",
    "        # add cluster info to df\n",
    "        jobs=df['jobtitle'].astype(str).tolist()\n",
    "        jobs_clusters=DataTransformer.jobs_pipe.predict(jobs)\n",
    "        df['job_cluster']=jobs_clusters\n",
    "        \n",
    "        return df\n",
    "    ###\n",
    "    def transform(self,fit=False,labels=False,remove_info=False):\n",
    "        \n",
    "        self.df_education=self.get_edu_clusters(self.df_education,fit,remove_info)\n",
    "        self.df_seniority=self.get_position_clusters(self.df_seniority,fit)\n",
    "        \n",
    "        self.df_positions,self.df_education,self.df_seniority=self.transform_pd(self.df_positions, self.df_education,self.df_seniority,fit)\n",
    "        merged=self.merge_pd(self.df_positions,self.df_education,self.df_seniority,labels,remove_info)\n",
    "        \n",
    "        \n",
    "        return self.format_to_size(merged,labels)\n",
    "    \n",
    "    \n",
    "dataTransformer = DataTransformer(df_positions_train,df_education_train,df_seniority_train)\n",
    "train_data =dataTransformer.transform(fit=True,labels=True,remove_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols=['enddate_y','startdate_y','seniority','enddate_x','startdate_x','major_cluster']\n",
    "\n",
    "\n",
    "X=np.asarray(train_data[feature_cols].values.tolist())\n",
    "# matrix dim for emsemble method\n",
    "X=np.reshape(X, (len(X), X.shape[1]*X.shape[2]))\n",
    "\n",
    "# gets rid of future warning\n",
    "y=np.asarray(train_data[['label']]).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "#### After testing multiple models from sklearn, ensemble methods were the ones that worked the best\n",
    "\n",
    "#### We are going to investigate Neural networks later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split( X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# defining parameter range \n",
    "# Number of trees in random forest\n",
    "n_estimators = [200]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x*10) for x in range(1,5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "njobs=[-1]\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "              'n_jobs' : njobs\n",
    "                }\n",
    "   \n",
    "#grid = RandomizedSearchCV(RandomForestRegressor(), param_grid, refit = True, verbose = 10) \n",
    "   \n",
    "# fitting the model for grid search \n",
    "#grid.fit(X_train, y_train) \n",
    " \n",
    "# print best parameter after tuning \n",
    "#print(grid.best_params_) \n",
    "#grid_predictions = grid.predict(X_val) \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score 0.8487480640218668\n",
      "explained_variance_score 0.8487680760488888\n",
      "validation mean abs error 2.1513609278220915\n",
      "validation RMSE 4.034577903691025\n"
     ]
    }
   ],
   "source": [
    "# Scaler not necessary here but it make it easier to change model, when we have to use it\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('RandomForestRegressor', RandomForestRegressor(n_estimators=200,n_jobs=-1,min_samples_split=2,max_features='auto',min_samples_leaf=2,bootstrap=True,max_depth=40))])\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred = pipe.predict(X_val)\n",
    "print('r2 score', pipe.score(X_val,y_val))\n",
    "print('explained_variance_score',explained_variance_score(y_val, y_pred))\n",
    "print('validation mean abs error',mean_absolute_error(y_val, y_pred))\n",
    "print('validation RMSE',mean_squared_error(y_val, y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\AppData\\Local\\Temp/ipykernel_13508/2790559756.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['major_cluster']=major_clusters\n",
      "C:\\Users\\nguye\\AppData\\Local\\Temp/ipykernel_13508/2790559756.py:216: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['job_cluster']=jobs_clusters\n",
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\core\\indexing.py:1951: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[selected_item_labels] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing dates and text format \n",
      "\n",
      "completing position end dates...\n",
      "added 64030 values \n",
      "\n",
      "completing education end dates...\n",
      "added 5715 values \n",
      "\n",
      "completing education start dates...\n",
      "average length:  3.608725981355846 yrs\n",
      "added 879 values \n",
      "\n",
      "completing position end dates...\n",
      "average length:  3.962133030775858 yrs\n",
      "added 24 values \n",
      "\n",
      "completing education remaining dates...\n",
      "average start date:  2003.2742277481523 yrs\n",
      "average end date:  2006.882953729532 yrs\n",
      "added 18144 values \n",
      "\n",
      "completing position remaining dates...\n",
      "average start date:  2010.1799402438633 yrs\n",
      "average end date:  2014.1420732741742 yrs\n",
      "added 6696 values \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\core\\generic.py:6392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n",
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "dataTransformer = DataTransformer(df_positions_test,df_education_test,df_seniority_test)\n",
    "test_data =dataTransformer.transform(fit=False,labels=False,remove_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.asarray(test_data[feature_cols].values.tolist())\n",
    "# matrix dim for emsemble method\n",
    "X_test=np.reshape(X_test, (len(X_test), X_test.shape[1]*X_test.shape[2]))\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66069"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = y_pred\n",
    "test_data['true_or_predicted']=False\n",
    "\n",
    "train_data['true_or_predicted']=True\n",
    "\n",
    "df_all=pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "now = datetime.now()\n",
    "today=now.year+now.month/13\n",
    "\n",
    "df_all['age']=(today-df_all['label']).astype(int)\n",
    "df_all=df_all[['age','true_or_predicted']]\n",
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>true_or_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23751</th>\n",
       "      <td>++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>83</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23752</th>\n",
       "      <td>++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>58</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23753</th>\n",
       "      <td>++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23754</th>\n",
       "      <td>++7kB6m0hI1TgAPmyY1X6A5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23755</th>\n",
       "      <td>++9DtAOTiRRvECoMIpKbmg4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>34</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89815</th>\n",
       "      <td>zzZdW3VGODRxRl2025ZR2w5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>34</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89816</th>\n",
       "      <td>zzrNxfUzwZXNkSs15haLyA4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89817</th>\n",
       "      <td>zzrbQXjc2yHwbWjtQ9F3mg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89818</th>\n",
       "      <td>zzuZVPanBvW09lNk1C3h+Q5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89819</th>\n",
       "      <td>zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66069 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 user_id  age  \\\n",
       "23751  ++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...   83   \n",
       "23752  ++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...   58   \n",
       "23753  ++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...   27   \n",
       "23754  ++7kB6m0hI1TgAPmyY1X6A5+2cvffV/mNepQVJd0smgtpB...   40   \n",
       "23755  ++9DtAOTiRRvECoMIpKbmg4ZM3TcQvn1bQ/jHgHWG0kf/b...   34   \n",
       "...                                                  ...  ...   \n",
       "89815  zzZdW3VGODRxRl2025ZR2w5+2cvffV/mNepQVJd0smgtpB...   34   \n",
       "89816  zzrNxfUzwZXNkSs15haLyA4ZM3TcQvn1bQ/jHgHWG0kf/b...   56   \n",
       "89817  zzrbQXjc2yHwbWjtQ9F3mg5+2cvffV/mNepQVJd0smgtpB...   32   \n",
       "89818  zzuZVPanBvW09lNk1C3h+Q5+2cvffV/mNepQVJd0smgtpB...   31   \n",
       "89819  zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...   25   \n",
       "\n",
       "       true_or_predicted  \n",
       "23751              False  \n",
       "23752              False  \n",
       "23753              False  \n",
       "23754              False  \n",
       "23755              False  \n",
       "...                  ...  \n",
       "89815              False  \n",
       "89816              False  \n",
       "89817              False  \n",
       "89818              False  \n",
       "89819              False  \n",
       "\n",
       "[66069 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamere=df_all[~df_all['true_or_predicted']]\n",
    "tamere.reset_index(drop=True)\n",
    "tamere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      major   startdate     enddate\n",
      "99508    BS  1949-01-01  1953-01-01\n",
      "                                jobtitle   startdate enddate\n",
      "41525  former_owner_presently_consultant  1953-01-01     NaN\n",
      "                                jobtitle  seniority\n",
      "90141  former_owner_presently_consultant   7.064817\n",
      "=========\n",
      "                   major   startdate     enddate\n",
      "92083  BS in Electronics  1973-01-01  1978-01-01\n",
      "92505                NaN  1984-01-01  1987-01-01\n",
      "                                                 jobtitle   startdate  \\\n",
      "9781                    owner_|_computer_network_security  1993-06-01   \n",
      "106525  design_engineer_|_mechanical_industrial_engine...  1984-10-01   \n",
      "\n",
      "           enddate  \n",
      "9781           NaN  \n",
      "106525  1989-05-01  \n",
      "                                                 jobtitle  seniority\n",
      "71129   design_engineer_|_mechanical_industrial_engine...   3.331507\n",
      "222292                  owner_|_computer_network_security   7.334247\n",
      "=========\n",
      "                      major   startdate     enddate\n",
      "133238  Master Grande Ecole  2013-01-01  2016-01-01\n",
      "                                                 jobtitle   startdate  \\\n",
      "228241  digital_communication_press_relations_|_cosmetics  2016-07-01   \n",
      "228215             assistant_business_development_manager  2014-06-01   \n",
      "176460  trainee_digital_brand_commerce_crm_implementat...  2018-04-01   \n",
      "237149  trainee_digital_brand_commerce_digital_plannin...  2017-09-01   \n",
      "200727  sales_business_development_executive_|_events_...  2017-06-01   \n",
      "30076                     event_promoter_public_relations  2015-07-01   \n",
      "220136  trainee_digital_brand_commerce_membership_prog...  2018-09-01   \n",
      "23563          freelance_author_lucifer_manga_books_serie  2016-12-01   \n",
      "213101  digital_communication_social_medias_activation...  2016-11-01   \n",
      "\n",
      "           enddate  \n",
      "228241  2016-10-01  \n",
      "228215  2014-08-01  \n",
      "176460         NaN  \n",
      "237149  2018-03-01  \n",
      "200727  2017-08-01  \n",
      "30076   2015-08-01  \n",
      "220136         NaN  \n",
      "23563   2017-08-01  \n",
      "213101  2017-05-01  \n",
      "                                                 jobtitle  seniority\n",
      "399946  digital_communication_social_medias_activation...   4.307247\n",
      "220871                    event_promoter_public_relations   1.908356\n",
      "351436  trainee_digital_brand_commerce_membership_prog...   4.307247\n",
      "354892  trainee_digital_brand_commerce_digital_plannin...   4.307247\n",
      "111281  sales_business_development_executive_|_events_...   3.439384\n",
      "44321          freelance_author_lucifer_manga_books_serie   4.020771\n",
      "333670  digital_communication_press_relations_|_cosmetics   4.307247\n",
      "24521              assistant_business_development_manager   3.787412\n",
      "372408  trainee_digital_brand_commerce_crm_implementat...   4.307247\n",
      "=========\n",
      "                  major   startdate     enddate\n",
      "106134  Master's Degree  2008-01-01         NaN\n",
      "106774              NaN  2012-01-01  2013-01-01\n",
      "106190               BS  1996-01-01  1997-01-01\n",
      "                                                 jobtitle   startdate  \\\n",
      "311981                                           director  2012-08-01   \n",
      "324558                                  preschool_teacher  2011-06-01   \n",
      "280513            sr_research_associate_|_pharmaceuticals  1999-09-01   \n",
      "320187           validation_scientist_|_computer_software  2014-09-01   \n",
      "72818                           clinical_data_coordinator  2004-09-01   \n",
      "245906   computational_chemistry_intern_|_pharmaceuticals  1997-12-01   \n",
      "330491                                 substitute_teacher  2011-01-01   \n",
      "288582  validation_scientist_bristol_myers_squibb_co_|...  2005-05-01   \n",
      "351507           validation_scientist_|_computer_software  2008-03-01   \n",
      "\n",
      "           enddate  \n",
      "311981  2014-02-01  \n",
      "324558  2012-08-01  \n",
      "280513  2003-12-01  \n",
      "320187         NaN  \n",
      "72818   2005-05-01  \n",
      "245906  1998-12-01  \n",
      "330491  2011-05-01  \n",
      "288582  2007-09-01  \n",
      "351507  2012-12-01  \n",
      "                                                 jobtitle  seniority\n",
      "385257   computational_chemistry_intern_|_pharmaceuticals   4.142466\n",
      "343977  validation_scientist_bristol_myers_squibb_co_|...   2.750685\n",
      "20790                           clinical_data_coordinator   3.610914\n",
      "375097           validation_scientist_|_computer_software   4.713699\n",
      "231173            sr_research_associate_|_pharmaceuticals   5.046575\n",
      "248331                                           director   7.421538\n",
      "11467                                  substitute_teacher   2.039746\n",
      "343355                                  preschool_teacher   2.855363\n",
      "=========\n",
      "         major   startdate     enddate\n",
      "29395  B. Tech  2002-01-01  2006-01-01\n",
      "29488      MSc  2009-01-01  2010-01-01\n",
      "                                                jobtitle   startdate  \\\n",
      "149353                      business_development_manager  2007-01-01   \n",
      "163857  business_development_manager_|_computer_software  2007-07-01   \n",
      "136336          post_graduate_student_|_higher_education  2009-01-01   \n",
      "\n",
      "           enddate  \n",
      "149353  2007-07-01  \n",
      "163857  2008-12-01  \n",
      "136336         NaN  \n",
      "                                                jobtitle  seniority\n",
      "411244                      business_development_manager   5.325018\n",
      "251617  business_development_manager_|_computer_software   5.002740\n",
      "25815           post_graduate_student_|_higher_education   3.793151\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "for i in range (23751,23751+5):\n",
    "    user=tamere.loc[i,'user_id']\n",
    "    get_user_history(user)\n",
    "    print('=========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN and perception with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping the same split for consistency\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "\n",
    "X_train=np.reshape(X_train, (len(X_train),original_shape[1],original_shape[2]))\n",
    "X_train=np.swapaxes(X_train,1,2)\n",
    "X_val=scaler.transform(X_val) # no fit\n",
    "X_val=np.reshape(X_val, (len(X_val),original_shape[1],original_shape[2]))\n",
    "X_val=np.swapaxes(X_val,1,2)\n",
    "X_val.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_scaler=StandardScaler()\n",
    "y_train=label_scaler.fit_transform(y_train.reshape(-1, 1)).reshape(-1)\n",
    "y_val=label_scaler.transform(y_val.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential() \n",
    "model.add(layers.Input((original_shape[2],original_shape[1])))\n",
    "model.add(layers.LSTM(1024, return_sequences=True))\n",
    "model.add(layers.LSTM(256, input_shape=X.shape, return_sequences=False))\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "print(model.summary() )\n",
    "model.compile(loss='mean_squared_error',\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0003),\n",
    "              metrics=['accuracy']) \n",
    "history_LSTM = model.fit(X_train, y_train, validation_data=(X_val, y_val),batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model.predict(X_val)\n",
    "y_pred2 =label_scaler.inverse_transform(y_pred2.reshape(-1,1)).reshape(-1)\n",
    "y_true =label_scaler.inverse_transform(y_val.reshape(-1,1)).reshape(-1)\n",
    "accs=[abs(y_pred2[i]-y_true[i]) for i in range(len(y_true))]\n",
    "sum(accs)/len(accs)\n",
    "print('RMSE',mean_squared_error(y_true, y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.reshape(X_train, (len(X_train),original_shape[1]*original_shape[2]))\n",
    "X_val=np.reshape(X_val, (len(X_val),original_shape[1]*original_shape[2]))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "model_perceptron = tf.keras.Sequential() \n",
    "model_perceptron.add(layers.Input((original_shape[1]*original_shape[2])))\n",
    "model_perceptron.add(layers.Dense(30,activation='relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)))\n",
    "model_perceptron.add(layers.Dense(20,activation='relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)))\n",
    "model_perceptron.add(layers.Dense(10,activation='relu'))\n",
    "model_perceptron.add(layers.Dense(5,activation='relu'))\n",
    "model_perceptron.add(layers.Dense(5,activation='relu'))\n",
    "model_perceptron.add(layers.Dense(10,activation='relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)))\n",
    "model_perceptron.add(layers.Dense(1))\n",
    "\n",
    "print(model_perceptron.summary() )\n",
    "model_perceptron.compile(loss='mean_squared_error',\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001),\n",
    "              metrics=['accuracy']) \n",
    "history_perceptron = model_perceptron.fit(X_train, y_train, validation_data=(X_val, y_val),batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val =label_scaler.inverse_transform(y_val.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = model_perceptron.predict(X_val)\n",
    "print(y_pred3.shape)\n",
    "y_pred3 =label_scaler.inverse_transform(y_pred3.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "accs=[abs(y_pred3[i]-y_val[i]) for i in range(len(y_val))]\n",
    "print('avg abs err:',sum(accs)/len(accs))\n",
    "print('RMSE',mean_squared_error(y_val, y_pred3,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on all the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaination end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majors_pipe=None\n",
    "jobs_pipe=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmean test, bachelor to be removed\n",
    "# data leakage but intuitively will improve the final accuracy\n",
    "\n",
    "texts=df_education_train['major'].tolist()\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text=[]\n",
    "for t in texts:\n",
    "    if (not pd.isnull(t)) and (not 'bachelor' in t.lower() or random.random()<0.33): # removing one third of bechelors to match the rest of the data set\n",
    "        processed_text.append(t)\n",
    "len(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 10\n",
    "majors_pipe = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 4))), ('majors_model', KMeans(n_clusters=number_of_clusters, init='k-means++', max_iter=50, n_init=2))])\n",
    "majors_pipe.fit(processed_text)\n",
    "\n",
    "\n",
    "\n",
    "order_centroids = majors_pipe['majors_model'].cluster_centers_.argsort()[:, ::-1]\n",
    "terms = majors_pipe['vectorizer'].get_feature_names_out()\n",
    "\n",
    "for i in range(0,number_of_clusters):\n",
    "    print(\"class %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cluster_edu(df):\n",
    "    majors=df['major'].astype(str).tolist()\n",
    "    major_clusters=majors_pipe.predict(majors)\n",
    "    df['major_cluster']=major_clusters\n",
    "add_cluster_edu(df_education_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cluster_edu(df_education_val)\n",
    "add_cluster_edu(df_education_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education_test.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education_train[df_education_train['major_cluster']==3].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Usefull functions\n",
    "\n",
    "\n",
    "def to_lower_case(df,col):\n",
    "    df[col] =  df[col].str.lower()\n",
    "    return df \n",
    "def to_datetime(df,col):\n",
    "    df[col] =  pd.to_datetime(df[col], format='%Y-%m-%d')\n",
    "    df[col] =  df[col].dt.year+ df[col].dt.month/13\n",
    "    return df\n",
    "\n",
    "print('changing dates and text format \\n')\n",
    "\n",
    "df_education = df_education.pipe(to_lower_case, col='major').pipe(to_datetime,col='startdate').pipe(to_datetime,col='enddate')\n",
    "df_positions = df_positions.pipe(to_lower_case, col='jobtitle').pipe(to_datetime,col='startdate').pipe(to_datetime,col='enddate')\n",
    "df_seniority = df_seniority.pipe(to_lower_case, col='jobtitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values\n",
    "\n",
    "def fill_end_dates(df):\n",
    "    now = datetime.now()\n",
    "    today=now.year+now.month/13\n",
    "    #print('today',today)\n",
    "    before=df['enddate'].isna().sum()\n",
    "    df['enddate']=df.apply(lambda row: today if pd.isnull(row['enddate']) and (not pd.isnull(row['startdate'])) else row['enddate'],axis=1)\n",
    "    print('added',before-df['enddate'].isna().sum(),'values \\n')\n",
    "    return df\n",
    "\n",
    "print('completing position end dates...')\n",
    "\n",
    "df_positions=fill_end_dates(df_positions)\n",
    "print('completing education end dates...')\n",
    "df_education=fill_end_dates(df_education)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_startdate(df):\n",
    "    df_nona=df[['startdate','enddate']].dropna(how='all',inplace=False)\n",
    "    df_nona['length']=df_nona['enddate']-df_nona['startdate']\n",
    "    avg_length=df_nona[\"length\"].mean()\n",
    "    print('average length: ',avg_length,'yrs')\n",
    "    before=df['startdate'].isna().sum()\n",
    "    df['startdate'].fillna(df['enddate']-avg_length, inplace=True)\n",
    "    print('added',before-df['startdate'].isna().sum(),'values \\n')\n",
    "    return df\n",
    "print('completing education start dates...')\n",
    "df_education=fill_startdate(df_education)\n",
    "print('completing position end dates...')\n",
    "df_positions=fill_startdate(df_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dates_missingall(df):\n",
    "    df_nona=df[['startdate','enddate']].dropna(how='all',inplace=False)\n",
    "    #df_nona['length']=df_nona['enddate']-df_nona['startdate']\n",
    "    avg_start=df_nona[\"startdate\"].mean()\n",
    "    avg_end=df_nona[\"enddate\"].mean()\n",
    "    print('average start date: ',avg_start,'yrs')\n",
    "    print('average end date: ',avg_end,'yrs')\n",
    "    before=df['startdate'].isna().sum()\n",
    "    df['startdate'].fillna(avg_start, inplace=True)\n",
    "    df['enddate'].fillna(avg_end, inplace=True)\n",
    "    print('added',before-df['startdate'].isna().sum(),'values \\n')\n",
    "    return df  \n",
    "print('completing education remaining dates...')\n",
    "df_education=fill_dates_missingall(df_education)\n",
    "print('completing position remaining dates...')\n",
    "df_positions=fill_dates_missingall(df_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing education and positions\n",
    "df_positions['jobtitle'].fillna('positions', inplace=True)\n",
    "df_seniority['jobtitle'].fillna('positions', inplace=True)\n",
    "df_education['major'].fillna('education', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seniority['seniority'].fillna((df_seniority['seniority'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting by startdate for later:\n",
    "df_positions.sort_values(by=['startdate'],inplace=True)\n",
    "df_education.sort_values(by=['startdate'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_education.isna().sum())\n",
    "print(df_seniority.isna().sum())\n",
    "df_positions.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_positions.merge(df_seniority, how='right',left_on=['user_id','jobtitle'], right_on=['user_id','jobtitle']) # only position lacks users, hence right join\n",
    "df=df.groupby('user_id').agg(lambda x: list(x))\n",
    "df_education_temp=df_education.groupby('user_id').agg(lambda x: list(x))\n",
    "df=df.merge(df_education_temp, how='outer', left_on='user_id', right_on='user_id')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id ='++NWXZ8bW3I7VaXV77eiwA5+2cvffV/mNepQVJd0smgtpBr4MGMFJQ=='\n",
    "print(test_id)\n",
    "get_user_history(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count={'jobtitle': 0,'startdate_x':0,'enddate_x':0,'startdate_y':0,'startdate_y':0,'major':0,'seniority':0}\n",
    "mask=[]\n",
    "\n",
    "for rowIndex, row in df.iterrows(): \n",
    "    ismissing=False\n",
    "    for i, value in row.items():\n",
    "        for n in value:\n",
    "            if (pd.isnull(n)):\n",
    "                ismissing=True\n",
    "                missing_count[i]=missing_count[i]+1\n",
    "    if ismissing:\n",
    "        mask.append(True)\n",
    "    else:\n",
    "        mask.append(False)\n",
    "missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in mask if x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['startdate_x']=df.apply(lambda row: [max(row['enddate_y']) if pd.isnull(date) else date for date in row['startdate_x'] ] ,axis=1)\n",
    "now = datetime.now()\n",
    "today=now.year+now.month/13\n",
    "df['enddate_x']=df['enddate_x'].apply(lambda dates: [today if pd.isnull(date) else date for date in dates])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# < 1h\n",
    "# Make basic version with no nlp\n",
    "\n",
    "# Clean code, rearrange in clear steps : panda join, slipt, preprocessing, training\n",
    "# clean Variable names (make sure it still works), add comments\n",
    "\n",
    "\n",
    "# ~1h\n",
    "# make the diy nlp with most occuring degree terms\n",
    "# test if imporvements\n",
    "\n",
    "\n",
    "\n",
    "# pandas output\n",
    "#email to ask \n",
    "\n",
    "# test rnn\n",
    "\n",
    "# improvements: cross check with friend list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b4ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785a570",
   "metadata": {},
   "source": [
    "# 0. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f90003",
   "metadata": {},
   "source": [
    "Downloading the data, so that we don't have to do it again if we restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a5f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data downloaded\n"
     ]
    }
   ],
   "source": [
    "urls=[\"https://info0.s3.us-east-2.amazonaws.com/recruitment/positions.csv\",\"https://info0.s3.us-east-2.amazonaws.com/recruitment/education.csv\",\"https://info0.s3.us-east-2.amazonaws.com/recruitment/jobtitle_seniority.csv\"]\n",
    "\n",
    "# data folder, create if it does not exists\n",
    "os.makedirs('./data/', exist_ok=True) \n",
    "    \n",
    "# if .csv file does not exists, download it\n",
    "for url in urls:\n",
    "    path='./data/' + os.path.basename(url)\n",
    "    if not os.path.exists(path):\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "        \n",
    "print('data downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c1a1d",
   "metadata": {},
   "source": [
    "Loading the data into pandas data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d89c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positions = pd.read_csv(\"./data/positions.csv\")\n",
    "df_education = pd.read_csv(\"./data/education.csv\")\n",
    "df_seniority = pd.read_csv(\"./data/jobtitle_seniority.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0837c4",
   "metadata": {},
   "source": [
    "# 1. Checking the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Individual data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36a0326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>jobtitle</th>\n",
       "      <th>startdate</th>\n",
       "      <th>enddate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162425</th>\n",
       "      <td>zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>student_senior_service_college</td>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>2013-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15089</th>\n",
       "      <td>zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>command_general_staff_college</td>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2005-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146169</th>\n",
       "      <td>zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>engineer_company_brigade_staff_trainer</td>\n",
       "      <td>2002-04-01</td>\n",
       "      <td>2004-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15923</th>\n",
       "      <td>zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>student_engineer_officer</td>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>1998-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20486</th>\n",
       "      <td>zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>company_commander_battalion_battalion_assistant</td>\n",
       "      <td>1998-11-01</td>\n",
       "      <td>2002-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309939</th>\n",
       "      <td>zzrNxfUzwZXNkSs15haLyA4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>partner_head_private_client_department_|_law_p...</td>\n",
       "      <td>1992-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338504</th>\n",
       "      <td>zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...</td>\n",
       "      <td>coordinador_de_personal_embarcado</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334912</th>\n",
       "      <td>zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...</td>\n",
       "      <td>operador_|_logistics_supply_chain</td>\n",
       "      <td>2014-11-01</td>\n",
       "      <td>2015-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359048</th>\n",
       "      <td>zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...</td>\n",
       "      <td>supervisor_de_personal_|_maritime</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>2017-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248375</th>\n",
       "      <td>zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...</td>\n",
       "      <td>supervisor_de_personal_|_logistics_supply_chain</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>2016-05-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  user_id  \\\n",
       "162425  zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "15089   zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "146169  zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "15923   zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "20486   zzdHAVxl9iQrwom22S/FLg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "309939  zzrNxfUzwZXNkSs15haLyA4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "338504  zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...   \n",
       "334912  zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...   \n",
       "359048  zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...   \n",
       "248375  zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...   \n",
       "\n",
       "                                                 jobtitle   startdate  \\\n",
       "162425                     student_senior_service_college  2012-08-01   \n",
       "15089                       command_general_staff_college  2004-07-01   \n",
       "146169             engineer_company_brigade_staff_trainer  2002-04-01   \n",
       "15923                            student_engineer_officer  1998-01-01   \n",
       "20486     company_commander_battalion_battalion_assistant  1998-11-01   \n",
       "309939  partner_head_private_client_department_|_law_p...  1992-01-01   \n",
       "338504                  coordinador_de_personal_embarcado  2017-12-01   \n",
       "334912                  operador_|_logistics_supply_chain  2014-11-01   \n",
       "359048                  supervisor_de_personal_|_maritime  2016-05-01   \n",
       "248375    supervisor_de_personal_|_logistics_supply_chain  2015-11-01   \n",
       "\n",
       "           enddate  \n",
       "162425  2013-06-01  \n",
       "15089   2005-06-01  \n",
       "146169  2004-06-01  \n",
       "15923   1998-10-01  \n",
       "20486   2002-03-01  \n",
       "309939         NaN  \n",
       "338504         NaN  \n",
       "334912  2015-09-01  \n",
       "359048  2017-09-01  \n",
       "248375  2016-05-01  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positions=df_positions.sort_values(by=['user_id'])\n",
    "df_positions.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca9cec",
   "metadata": {},
   "source": [
    "#### Looking at the dates, linkedIn only has the year and month information, so date formatting is YYYY-MM-DD\n",
    "#### Position titles and fields are seperated by '\\_|\\_', words are seperated by '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e299739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>major</th>\n",
       "      <th>startdate</th>\n",
       "      <th>enddate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99508</th>\n",
       "      <td>++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>BS</td>\n",
       "      <td>1949-01-01</td>\n",
       "      <td>1953-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92083</th>\n",
       "      <td>++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>BS in Electronics</td>\n",
       "      <td>1973-01-01</td>\n",
       "      <td>1978-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92505</th>\n",
       "      <td>++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>1987-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133238</th>\n",
       "      <td>++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>Master Grande Ecole</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5126</th>\n",
       "      <td>++6zEVtPCi83vpPTHSY2Vg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>Bachelor of Science (B.Sc.) (ED)</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2006-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  user_id  \\\n",
       "99508   ++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "92083   ++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "92505   ++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "133238  ++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...   \n",
       "5126    ++6zEVtPCi83vpPTHSY2Vg5+2cvffV/mNepQVJd0smgtpB...   \n",
       "\n",
       "                                   major   startdate     enddate  \n",
       "99508                                 BS  1949-01-01  1953-01-01  \n",
       "92083                  BS in Electronics  1973-01-01  1978-01-01  \n",
       "92505                                NaN  1984-01-01  1987-01-01  \n",
       "133238               Master Grande Ecole  2013-01-01  2016-01-01  \n",
       "5126    Bachelor of Science (B.Sc.) (ED)  2001-01-01  2006-01-01  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_education=df_education.sort_values(by=['user_id'])\n",
    "df_education.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c69fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>jobtitle</th>\n",
       "      <th>seniority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90141</th>\n",
       "      <td>++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>former_owner_presently_consultant</td>\n",
       "      <td>7.064817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71129</th>\n",
       "      <td>++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>design_engineer_|_mechanical_industrial_engine...</td>\n",
       "      <td>3.331507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222292</th>\n",
       "      <td>++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>owner_|_computer_network_security</td>\n",
       "      <td>7.334247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399946</th>\n",
       "      <td>++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>digital_communication_social_medias_activation...</td>\n",
       "      <td>4.307247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220871</th>\n",
       "      <td>++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>event_promoter_public_relations</td>\n",
       "      <td>1.908356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  user_id  \\\n",
       "90141   ++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "71129   ++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "222292  ++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...   \n",
       "399946  ++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...   \n",
       "220871  ++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...   \n",
       "\n",
       "                                                 jobtitle  seniority  \n",
       "90141                   former_owner_presently_consultant   7.064817  \n",
       "71129   design_engineer_|_mechanical_industrial_engine...   3.331507  \n",
       "222292                  owner_|_computer_network_security   7.334247  \n",
       "399946  digital_communication_social_medias_activation...   4.307247  \n",
       "220871                    event_promoter_public_relations   1.908356  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seniority=df_seniority.sort_values(by=['user_id'])\n",
    "df_seniority.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e8fc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 226184 entries, 99508 to 169422\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   user_id    226184 non-null  object\n",
      " 1   major      162346 non-null  object\n",
      " 2   startdate  197556 non-null  object\n",
      " 3   enddate    190658 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 8.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 377585 entries, 41525 to 248375\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   user_id    377585 non-null  object\n",
      " 1   jobtitle   376136 non-null  object\n",
      " 2   startdate  368526 non-null  object\n",
      " 3   enddate    270354 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 14.4+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 416295 entries, 90141 to 126315\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    416295 non-null  object \n",
      " 1   jobtitle   414290 non-null  object \n",
      " 2   seniority  416295 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 12.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_education.info())\n",
    "print(df_positions.info())\n",
    "print(df_seniority.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not all data frames have 100k users, all have to take this into account when merging the data frames.\n",
    "We will assume that we want to predict the age of all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81464\n",
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# give the number of DISTINCT users\n",
    "def count_users(df):\n",
    "    return len(pd.unique(df['user_id']))\n",
    "\n",
    "print(count_users(df_positions))\n",
    "print(count_users(df_education))\n",
    "print(count_users(df_seniority))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6637ed",
   "metadata": {},
   "source": [
    "#### As expected a person can have multiple education, and position entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "781fde87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   major   startdate     enddate\n",
      "92083  BS in Electronics  1973-01-01  1978-01-01\n",
      "92505                NaN  1984-01-01  1987-01-01\n",
      "                                                 jobtitle   startdate  \\\n",
      "9781                    owner_|_computer_network_security  1993-06-01   \n",
      "106525  design_engineer_|_mechanical_industrial_engine...  1984-10-01   \n",
      "\n",
      "           enddate  \n",
      "9781           NaN  \n",
      "106525  1989-05-01  \n",
      "                                                 jobtitle  seniority\n",
      "71129   design_engineer_|_mechanical_industrial_engine...   3.331507\n",
      "222292                  owner_|_computer_network_security   7.334247\n"
     ]
    }
   ],
   "source": [
    "test_id =df_education.iat[2,0] # this is an arbitrary user_id\n",
    "\n",
    "def get_user_history(user_id):\n",
    "    print(df_education[df_education['user_id'].str.contains(user_id,regex=False,na=False)][['major', 'startdate','enddate']])\n",
    "    print(df_positions[df_positions['user_id'].str.contains(user_id,regex=False,na=False)][['jobtitle', 'startdate','enddate']])\n",
    "    print(df_seniority[df_seniority['user_id'].str.contains(user_id,regex=False,na=False)][['jobtitle', 'seniority']])\n",
    "\n",
    "get_user_history(test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values for each df\n",
    "There are more missing values for end dates because it can the current position / education program of someone\n",
    "\n",
    "Missing values for dates are more important than for education/jobtittles to predict the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf4b1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id           0\n",
      "jobtitle       1449\n",
      "startdate      9059\n",
      "enddate      107231\n",
      "dtype: int64\n",
      "------\n",
      "user_id          0\n",
      "major        63838\n",
      "startdate    28628\n",
      "enddate      35526\n",
      "dtype: int64\n",
      "------\n",
      "user_id         0\n",
      "jobtitle     2005\n",
      "seniority       0\n",
      "dtype: int64\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "#Missing values\n",
    "print(df_positions.isna().sum(),'------',sep='\\n')\n",
    "print(df_education.isna().sum(),'------',sep='\\n')\n",
    "print(df_seniority.isna().sum(),'------',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing starting dates AND enddates are problematic because it makes it hard to impute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6518 unique users missing job date info in df_positions\n",
      "0 unique users missing seniority info\n",
      "18036 unique users missing education date info\n"
     ]
    }
   ],
   "source": [
    "df_missing_pos=df_positions[df_positions['enddate'].isna() & df_positions['startdate'].isna()]\n",
    "df_missing_sen=df_seniority[df_seniority['seniority'].isna() ]\n",
    "df_missing_edu=df_education[df_education['enddate'].isna() & df_education['startdate'].isna()]\n",
    "print(count_users(df_missing_pos),'unique users missing job date info in df_positions')\n",
    "print(count_users(df_missing_sen),'unique users missing seniority info')\n",
    "print(count_users(df_missing_edu),'unique users missing education date info')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is even more true if there are no dates for both the education and job entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981 unique users missing dates on all education and job info\n"
     ]
    }
   ],
   "source": [
    "df_missing_all=df_missing_pos.merge(df_missing_edu, how='inner',left_on=['user_id'], right_on=['user_id'])\n",
    "print(count_users(df_missing_all),'unique users missing dates on all education and job info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Splitting the data to get training data \n",
    "\n",
    "## dataprocessing will be done after to prevent data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Choosing the training Data\n",
    "\n",
    "People with highscool information have ages that are easy to predict since most people end highschool at the same age.\n",
    "The same is true for Bachelors. \n",
    "As a general rule, the ealier the education is in terms of degree, the better it is to predict the age because of the smaller variability.\n",
    "\n",
    "However as a general rule, masters happen after bachelors for example, and thus can still give information. Some text processing will be beneficial to encode the type of degree.\n",
    "\n",
    "Because there are more people with bachelor information (1/3 of the data) we will use the starting date of the bachelor to determine someone's age for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3368 unique users with a highschool info\n"
     ]
    }
   ],
   "source": [
    "mask_high = df_education.major.apply(lambda x: ('highschool' in str(x).lower()) or ('high school' in str(x).lower()) )\n",
    "print(count_users(df_education[mask_high]),'unique users with a highschool info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37463 unique users with a bachelor info\n"
     ]
    }
   ],
   "source": [
    "bachelors=['bachelor','bs ','ba ','b.s.','b.a.']\n",
    "mask_ba = df_education.major.apply(lambda x:  any(bachelor in str(x).lower() for bachelor in bachelors))\n",
    "print(count_users(df_education[mask_ba]),'unique users with a bachelor info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33931 unique users with a bachelor info containing exactly 'bachelor'\n"
     ]
    }
   ],
   "source": [
    "bachelors=['bachelor']\n",
    "mask_ba = df_education.major.apply(lambda x:  any(bachelor in str(x).lower() for bachelor in bachelors))\n",
    "print(count_users(df_education[mask_ba]),'unique users with a bachelor info containing exactly \\'bachelor\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the later as it is simpler to work with, and 1/3 of the data as training data is enough.\n",
    "##### we will use NLP and clustering to encode similer majors, taking into account abbreviations such as 'bs' or 'b.s.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the mask to get the list of users that are used for the traning data\n",
    "training_users=pd.unique(df_education[mask_ba]['user_id']).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Splitting the data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We split the users that have bachelor info into training data and validation data to later measure the performance of the models\n",
    "\n",
    "##### The rest of the users (about 2/3) is the testing data that has no label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the df into train, val and test data\n",
    "def split_df(users,df):\n",
    "    # 1/3 of the users with label is going to be used as val data\n",
    "    users_train,users_val=train_test_split( users, test_size = 0.3, random_state = 42)\n",
    "    # training, val, and testing data\n",
    "    df_train=df.loc[df['user_id'].isin(users_train)]\n",
    "    df_val=df.loc[df['user_id'].isin(users_val)]\n",
    "    df_test=df.loc[~ df['user_id'].isin(users)]\n",
    "    return df_train,df_val,df_test\n",
    "\n",
    "# we have three data frames to split\n",
    "df_positions_train,df_positions_val,df_positions_test=split_df(training_users,df_positions)\n",
    "df_education_train,df_education_val,df_education_test=split_df(training_users,df_education)\n",
    "df_seniority_train,df_seniority_val,df_seniority_test=split_df(training_users,df_seniority)\n",
    "\n",
    "\n",
    "# checking that we still have every user\n",
    "assert(count_users(df_positions_train)+count_users(df_positions_val)+count_users(df_positions_test)==count_users(df_positions)) #posisions does not have 100k users\n",
    "assert(count_users(df_education_train)+count_users(df_education_val)+count_users(df_education_test)==count_users(df_education))\n",
    "assert(count_users(df_seniority_train)+count_users(df_seniority_val)+count_users(df_seniority_test)==count_users(df_seniority))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "#import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the jobtitles that are '_' seperated\n",
    "def tokenize(text):\n",
    "    return(text.split('_'))\n",
    "\n",
    "class DataTransformer:\n",
    "    # to preven data leakage we use Class constants\n",
    "    #class constants only changed when fit=True\n",
    "    \n",
    "    # different durations used for imputation, only changed with fit=True\n",
    "    avg_length_job= avg_length_edu = avg_end_job = avg_start_job= avg_end_edu = avg_start_edu =0\n",
    "    \n",
    "    # pipelines for text processing, only trained with fit=True\n",
    "    majors_pipe = jobs_pipe = None\n",
    "    \n",
    "    def __init__(self,df_positions,df_education,df_seniority):\n",
    "        self.df_positions = df_positions\n",
    "        self.df_education=df_education\n",
    "        self.df_seniority = df_seniority\n",
    "        \n",
    "    ### pad arrays to fixed sixe for the df to be converted into a matrix\n",
    "    def format_to_size(self,df,labels=False):\n",
    "        def to_fixed_size(df,cols,size):\n",
    "            for col in cols:\n",
    "                # pad array with last element, or cut array to fixed sixe\n",
    "                df[col] = df[col].apply(lambda x: x[0:size] if (len(x)>size) else  (x + (size-len(x)) * [x[-1]]))\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        return to_fixed_size(df,['enddate_y','startdate_y','seniority','enddate_x','startdate_x','major_cluster','job_cluster'],10)\n",
    "    \n",
    "    ### Get labels with the bachelor info\n",
    "    def get_labels(self,df,remove_info=False):\n",
    "        df['label']=None\n",
    "        \n",
    "        # get the index of first substring occurence, we want the first bachelor degree \n",
    "        def get_first_index(mylist, substring):\n",
    "            for i, s in enumerate(mylist):\n",
    "                if substring in s:\n",
    "                      return i\n",
    "            return -1\n",
    "        # iter through the df\n",
    "        for index, row in df.iterrows():\n",
    "            i=get_first_index(row['major'],'bachelor')\n",
    "            # people start bachelors at 18\n",
    "            birth=row['startdate_y'][i]-18\n",
    "            \n",
    "            if i!=-1:\n",
    "                df.loc[index,'label'] = birth \n",
    "                \n",
    "                 # remove bachelor info for training \n",
    "                if remove_info and len(row['enddate_y'])>1 and random.random()>0.3:\n",
    "                    df.at[index,'enddate_y'] = row['enddate_y'][:i]+row['enddate_y'][i+1:]\n",
    "                    df.at[index,'startdate_y'] = row['startdate_y'][:i]+row['startdate_y'][i+1:]\n",
    "                    df.at[index,'major'] =  row['major'][:i]+row['major'][i+1:]\n",
    "            else:\n",
    "                print('ERROR COULD NOT GET LABEL FOR USER:', row.index.name)\n",
    "        return df\n",
    "    ### merge the positions,education and seniority df\n",
    "    def merge_pd(self,df_positions, df_education,df_seniority,labels=False,remove_info=False):\n",
    "        \n",
    "        # sorting by startdate for later when using the bachelor info\n",
    "        df_positions.sort_values(by=['startdate'],inplace=True)\n",
    "        df_education.sort_values(by=['startdate'],inplace=True)\n",
    "        \n",
    "        # Right merge because positions does not have all users\n",
    "        df=df_positions.merge(df_seniority, how='right',left_on=['user_id','jobtitle'], right_on=['user_id','jobtitle'])\n",
    "        df=df.groupby('user_id').agg(lambda x: list(x))\n",
    "        df_education_temp=df_education.groupby('user_id').agg(lambda x: list(x))\n",
    "        df=df.merge(df_education_temp, how='inner', left_on='user_id', right_on='user_id')\n",
    "        \n",
    "        df['startdate_x']=df.apply(lambda row: [max(row['enddate_y']) if pd.isnull(date) else date for date in row['startdate_x'] ] ,axis=1)\n",
    "        now = datetime.now()\n",
    "        today=now.year+now.month/13\n",
    "        df['enddate_x']=df['enddate_x'].apply(lambda dates: [today if pd.isnull(date) else date for date in dates])\n",
    "        \n",
    "        if labels: # add labels\n",
    "            df=self.get_labels(df,remove_info)\n",
    "        return df\n",
    "    ###\n",
    "    def transform_pd(self,df_positions, df_education,df_seniority,fit=False):\n",
    "        \n",
    "        def to_lower_case(df,col):\n",
    "            df.loc[:,col] =  df[col].str.lower()\n",
    "            return df \n",
    "        def to_datetime(df,col):\n",
    "            df.loc[:,col] =  pd.to_datetime(df[col], format='%Y-%m-%d')\n",
    "            df.loc[:,col] =  df[col].dt.year+ df[col].dt.month/13\n",
    "            return df\n",
    "        print('changing dates and text format \\n')\n",
    "\n",
    "        df_education = df_education.pipe(to_lower_case, col='major').pipe(to_datetime,col='startdate').pipe(to_datetime,col='enddate')\n",
    "        df_positions = df_positions.pipe(to_lower_case, col='jobtitle').pipe(to_datetime,col='startdate').pipe(to_datetime,col='enddate')\n",
    "        df_seniority = df_seniority.pipe(to_lower_case, col='jobtitle')\n",
    "        \n",
    "        def fill_end_dates(df):\n",
    "            now = datetime.now()\n",
    "            today=now.year+now.month/13\n",
    "            #print('today',today)\n",
    "            before=df['enddate'].isna().sum()\n",
    "            df.loc[:,'enddate']=df.loc[:,:].apply(lambda row: today if pd.isnull(row['enddate']) and (not pd.isnull(row['startdate'])) else row['enddate'],axis=1)\n",
    "            print('added',before-df['enddate'].isna().sum(),'values \\n')\n",
    "            return df\n",
    "\n",
    "        print('completing position end dates...')\n",
    "\n",
    "        df_positions=fill_end_dates(df_positions)\n",
    "        print('completing education end dates...')\n",
    "        df_education=fill_end_dates(df_education)\n",
    "\n",
    "        def fill_startdate(df,typ):\n",
    "            df_nona=df[['startdate','enddate']].dropna(how='all',inplace=False)\n",
    "            df_nona['length']=df_nona['enddate']-df_nona['startdate']\n",
    "            avg_length=0\n",
    "            if fit:\n",
    "                if typ=='edu':\n",
    "                    DataTransformer.avg_length_edu=avg_length=df_nona[\"length\"].mean()\n",
    "                else:\n",
    "                    DataTransformer.avg_length_job=avg_length=df_nona[\"length\"].mean()\n",
    "            else:\n",
    "                if typ=='edu':\n",
    "                    avg_length=DataTransformer.avg_length_edu\n",
    "                else:\n",
    "                    avg_length= DataTransformer.avg_length_job\n",
    "                \n",
    "            print('average length: ',avg_length,'yrs')\n",
    "            before=df['startdate'].isna().sum()\n",
    "            df['startdate'].fillna(df['enddate']-avg_length, inplace=True)\n",
    "            print('added',before-df['startdate'].isna().sum(),'values \\n')\n",
    "            return df\n",
    "        print('completing education start dates...')\n",
    "        df_education=fill_startdate(df_education,'edu')\n",
    "        print('completing position end dates...')\n",
    "        df_positions=fill_startdate(df_positions,'job')\n",
    "        \n",
    "        def fill_dates_missingall(df,typ):\n",
    "            df_nona=df[['startdate','enddate']].dropna(how='all',inplace=False)\n",
    "            #df_nona['length']=df_nona['enddate']-df_nona['startdate']\n",
    "            \n",
    "            avg_start=avg_end=0\n",
    "            if fit:\n",
    "                if typ=='edu':\n",
    "                    DataTransformer.avg_start_edu=avg_start=df_nona[\"startdate\"].mean()\n",
    "                    DataTransformer.avg_end_edu=avg_end=df_nona[\"enddate\"].mean()\n",
    "                else:\n",
    "                    DataTransformer.avg_start_job=avg_start=df_nona[\"startdate\"].mean()\n",
    "                    DataTransformer.avg_end_job=avg_end=df_nona[\"enddate\"].mean()\n",
    "            else:\n",
    "                if typ=='edu':\n",
    "                    avg_start=DataTransformer.avg_start_edu\n",
    "                    avg_end=DataTransformer.avg_end_edu\n",
    "                else:\n",
    "                    avg_start=DataTransformer.avg_start_job\n",
    "                    avg_end=DataTransformer.avg_end_job\n",
    "            \n",
    "                \n",
    "            print('average start date: ',avg_start,'yrs')\n",
    "            print('average end date: ',avg_end,'yrs')\n",
    "            before=df['startdate'].isna().sum()\n",
    "            df['startdate'].fillna(avg_start, inplace=True)\n",
    "            df['enddate'].fillna(avg_end, inplace=True)\n",
    "            print('added',before-df['startdate'].isna().sum(),'values \\n')\n",
    "            return df  \n",
    "        print('completing education remaining dates...')\n",
    "        df_education=fill_dates_missingall(df_education,'edu')\n",
    "        print('completing position remaining dates...')\n",
    "        df_positions=fill_dates_missingall(df_positions,'job')\n",
    "        \n",
    "        # filling missing education and positions\n",
    "        df_positions['jobtitle'].fillna('positions', inplace=True)\n",
    "        df_seniority['jobtitle'].fillna('positions', inplace=True)\n",
    "        df_education['major'].fillna('education', inplace=True)\n",
    "        \n",
    "        #filling missing seniority information\n",
    "        df_seniority['seniority'].fillna((df_seniority['seniority'].mean()), inplace=True)\n",
    "            \n",
    "        \n",
    "        return df_positions,df_education,df_seniority\n",
    "    \n",
    "    ####\n",
    "    def get_edu_clusters(self,df,fit,remove_info):\n",
    "        if fit:\n",
    "            processed_text=[]\n",
    "            for t in df['major'].tolist():\n",
    "                r=0\n",
    "                if remove_info:\n",
    "                    r=random.random() # if training or val data remove bachelor data to match the testing data bachelor ratio\n",
    "                if (not pd.isnull(t)) and (not 'bachelor' in t.lower() or r <0.33): # removing one third of bechelors to match the rest of the data set\n",
    "                    processed_text.append(t)\n",
    "            # clustering pipeline   \n",
    "\n",
    "            number_of_clusters = 10\n",
    "            DataTransformer.majors_pipe = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 4))), ('majors_model', KMeans(n_clusters=number_of_clusters, init='k-means++', max_iter=50, n_init=2))])\n",
    "            DataTransformer.majors_pipe.fit(processed_text)\n",
    "            order_centroids = DataTransformer.majors_pipe['majors_model'].cluster_centers_.argsort()[:, ::-1]\n",
    "            terms = DataTransformer.majors_pipe['vectorizer'].get_feature_names_out()\n",
    "            for i in range(0,number_of_clusters):\n",
    "                print(\"class %d:\" % i),\n",
    "                for ind in order_centroids[i, :10]:\n",
    "                    print(' %s' % terms[ind])\n",
    "                print('-------') \n",
    "                \n",
    "        # add cluster info to df\n",
    "        majors=df['major'].astype(str).tolist()\n",
    "        major_clusters=DataTransformer.majors_pipe.predict(majors)\n",
    "        df['major_cluster']=major_clusters\n",
    "        \n",
    "        return df\n",
    "    ###\n",
    "    def get_position_clusters(self,df,fit):\n",
    "        if fit:\n",
    "            processed_text=[]\n",
    "            for t in df['jobtitle'].tolist():\n",
    "           \n",
    "                if (not pd.isnull(t)): # removing one third of bechelors to match the rest of the data set\n",
    "                    processed_text.append(t)\n",
    "            # clustering pipeline   \n",
    "\n",
    "            number_of_clusters = 10\n",
    "            DataTransformer.jobs_pipe = Pipeline([('vectorizer', CountVectorizer(tokenizer=tokenize,ngram_range=(1, 4),stop_words=['|'])), ('jobs_model', KMeans(n_clusters=number_of_clusters, init='k-means++', max_iter=50, n_init=2))])\n",
    "            DataTransformer.jobs_pipe.fit(processed_text)\n",
    "            order_centroids = DataTransformer.jobs_pipe['jobs_model'].cluster_centers_.argsort()[:, ::-1]\n",
    "            terms = DataTransformer.jobs_pipe['vectorizer'].get_feature_names_out()\n",
    "            for i in range(0,number_of_clusters):\n",
    "                print(\"class %d:\" % i),\n",
    "                for ind in order_centroids[i, :10]:\n",
    "                    print(' %s' % terms[ind])\n",
    "                print('-------') \n",
    "                \n",
    "        # add cluster info to df\n",
    "        jobs=df['jobtitle'].astype(str).tolist()\n",
    "        jobs_clusters=DataTransformer.jobs_pipe.predict(jobs)\n",
    "        df['job_cluster']=jobs_clusters\n",
    "        \n",
    "        return df\n",
    "    ###\n",
    "    def transform(self,fit=False,labels=False,remove_info=False):\n",
    "        \n",
    "        self.df_education=self.get_edu_clusters(self.df_education,fit,remove_info)\n",
    "        self.df_seniority=self.get_position_clusters(self.df_seniority,fit)\n",
    "        \n",
    "        self.df_positions,self.df_education,self.df_seniority=self.transform_pd(self.df_positions, self.df_education,self.df_seniority,fit)\n",
    "        merged=self.merge_pd(self.df_positions,self.df_education,self.df_seniority,labels,remove_info)\n",
    "        \n",
    "        \n",
    "        return self.format_to_size(merged,labels)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### processing the training data\n",
    "Where we fit all the params for the text processing and imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0:\n",
      " geprüfter\n",
      " staatlich\n",
      " staatlich geprüfter\n",
      " techniker\n",
      " geprüfter techniker\n",
      " staatlich geprüfter techniker\n",
      " fachrichtung\n",
      " werkstofftechnik\n",
      " geprüfter techniker fachrichtung werkstofftechnik\n",
      " geprüfter techniker fachrichtung\n",
      "-------\n",
      "class 1:\n",
      " of\n",
      " bachelor\n",
      " bachelor of\n",
      " arts\n",
      " of arts\n",
      " bachelor of arts\n",
      " engineering\n",
      " ba\n",
      " bachelor of engineering\n",
      " of engineering\n",
      "-------\n",
      "class 2:\n",
      " ba\n",
      " economics\n",
      " and\n",
      " bachelors\n",
      " business\n",
      " accounting\n",
      " of\n",
      " finance\n",
      " and economics\n",
      " masters\n",
      "-------\n",
      "class 3:\n",
      " degree\n",
      " bachelor\n",
      " bachelor degree\n",
      " in\n",
      " degree in\n",
      " bachelor degree in\n",
      " business\n",
      " and\n",
      " of\n",
      " science\n",
      "-------\n",
      "class 4:\n",
      " administration\n",
      " of\n",
      " business\n",
      " of business\n",
      " of business administration\n",
      " business administration\n",
      " master of\n",
      " master\n",
      " master of business administration\n",
      " master of business\n",
      "-------\n",
      "class 5:\n",
      " science\n",
      " of\n",
      " of science\n",
      " bachelor of\n",
      " bachelor\n",
      " of science bs\n",
      " bs\n",
      " bachelor of science bs\n",
      " bachelor of science\n",
      " science bs\n",
      "-------\n",
      "class 6:\n",
      " of\n",
      " diploma\n",
      " doctor\n",
      " masters\n",
      " degree\n",
      " certificate\n",
      " doctor of\n",
      " in\n",
      " bachelors\n",
      " school\n",
      "-------\n",
      "class 7:\n",
      " of\n",
      " master\n",
      " master of\n",
      " science\n",
      " master of science\n",
      " of science\n",
      " arts\n",
      " of arts\n",
      " master of arts\n",
      " ms\n",
      "-------\n",
      "class 8:\n",
      " master\n",
      " degree\n",
      " master degree\n",
      " in\n",
      " master degree in\n",
      " degree in\n",
      " master 39\n",
      " master 39 degree\n",
      " 39 degree\n",
      " 39\n",
      "-------\n",
      "class 9:\n",
      " of\n",
      " science\n",
      " of science\n",
      " bachelor of\n",
      " bachelor\n",
      " bachelor of science\n",
      " sc\n",
      " science sc\n",
      " bsc\n",
      " of science bsc\n",
      "-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\AppData\\Local\\Temp/ipykernel_5768/1310676664.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['major_cluster']=major_clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0:\n",
      " time\n",
      " part time\n",
      " part\n",
      " education\n",
      " assistant\n",
      " higher education\n",
      " higher\n",
      " management\n",
      " sales\n",
      " manager\n",
      "-------\n",
      "class 1:\n",
      " financial\n",
      " services\n",
      " financial services\n",
      " manager\n",
      " analyst\n",
      " manager financial\n",
      " manager financial services\n",
      " analyst financial\n",
      " analyst financial services\n",
      " senior\n",
      "-------\n",
      "class 2:\n",
      " education\n",
      " higher education\n",
      " higher\n",
      " assistant\n",
      " assistant higher education\n",
      " assistant higher\n",
      " student\n",
      " research\n",
      " graduate\n",
      " research assistant\n",
      "-------\n",
      "class 3:\n",
      " founder\n",
      " co\n",
      " co founder\n",
      " op\n",
      " co op\n",
      " owner\n",
      " ceo\n",
      " co owner\n",
      " director\n",
      " president\n",
      "-------\n",
      "class 4:\n",
      " assistant\n",
      " manager\n",
      " research\n",
      " administrative\n",
      " administrative assistant\n",
      " assistant manager\n",
      " research assistant\n",
      " executive\n",
      " management\n",
      " office\n",
      "-------\n",
      "class 5:\n",
      " technology\n",
      " services\n",
      " information\n",
      " information technology\n",
      " technology services\n",
      " information technology services\n",
      " manager\n",
      " manager information\n",
      " manager information technology\n",
      " manager information technology services\n",
      "-------\n",
      "class 6:\n",
      " manager\n",
      " engineer\n",
      " intern\n",
      " sales\n",
      " senior\n",
      " director\n",
      " health\n",
      " management\n",
      " care\n",
      " analyst\n",
      "-------\n",
      "class 7:\n",
      " engineer\n",
      " information\n",
      " services\n",
      " information technology\n",
      " engineer information\n",
      " technology\n",
      " technology services\n",
      " engineer information technology\n",
      " engineer information technology services\n",
      " information technology services\n",
      "-------\n",
      "class 8:\n",
      " management\n",
      " consulting\n",
      " management consulting\n",
      " consultant\n",
      " consultant management\n",
      " consultant management consulting\n",
      " manager\n",
      " senior\n",
      " manager management\n",
      " manager management consulting\n",
      "-------\n",
      "class 9:\n",
      " marketing\n",
      " advertising\n",
      " marketing advertising\n",
      " manager\n",
      " manager marketing\n",
      " manager marketing advertising\n",
      " account\n",
      " director\n",
      " senior\n",
      " intern\n",
      "-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\AppData\\Local\\Temp/ipykernel_5768/1310676664.py:232: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['job_cluster']=jobs_clusters\n",
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\core\\indexing.py:1951: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[selected_item_labels] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing dates and text format \n",
      "\n",
      "completing position end dates...\n",
      "added 24017 values \n",
      "\n",
      "completing education end dates...\n",
      "added 1815 values \n",
      "\n",
      "completing education start dates...\n",
      "average length:  3.608725981355846 yrs\n",
      "added 398 values \n",
      "\n",
      "completing position end dates...\n",
      "average length:  3.962133030775858 yrs\n",
      "added 21 values \n",
      "\n",
      "completing education remaining dates...\n",
      "average start date:  2003.2742277481523 yrs\n",
      "average end date:  2006.882953729532 yrs\n",
      "added 6235 values \n",
      "\n",
      "completing position remaining dates...\n",
      "average start date:  2010.1799402438633 yrs\n",
      "average end date:  2014.1420732741742 yrs\n",
      "added 1635 values \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\core\\generic.py:6392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n",
      "C:\\Users\\nguye\\anaconda3\\envs\\envTF\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "feature_cols=['enddate_y','startdate_y','seniority','enddate_x','startdate_x','major_cluster','job_cluster']\n",
    "    \n",
    "dataTransformer = DataTransformer(df_positions_train,df_education_train,df_seniority_train)\n",
    "train_data =dataTransformer.transform(fit=True,labels=True,remove_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### processing the validation data\n",
    "We remove the info in the same manner as the training data for comparison purposes.\n",
    "\n",
    "The removal of some bachelor info is to make the model perform better on the test data where this info is not always available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\AppData\\Local\\Temp/ipykernel_5768/1310676664.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['major_cluster']=major_clusters\n",
      "C:\\Users\\nguye\\AppData\\Local\\Temp/ipykernel_5768/1310676664.py:232: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['job_cluster']=jobs_clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing dates and text format \n",
      "\n",
      "completing position end dates...\n",
      "added 10174 values \n",
      "\n",
      "completing education end dates...\n",
      "added 780 values \n",
      "\n",
      "completing education start dates...\n",
      "average length:  3.608725981355846 yrs\n",
      "added 135 values \n",
      "\n",
      "completing position end dates...\n",
      "average length:  3.962133030775858 yrs\n",
      "added 4 values \n",
      "\n",
      "completing education remaining dates...\n",
      "average start date:  2003.2742277481523 yrs\n",
      "average end date:  2006.882953729532 yrs\n",
      "added 2837 values \n",
      "\n",
      "completing position remaining dates...\n",
      "average start date:  2010.1799402438633 yrs\n",
      "average end date:  2014.1420732741742 yrs\n",
      "added 679 values \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataTransformer = DataTransformer(df_positions_val,df_education_val,df_seniority_val)\n",
    "val_data =dataTransformer.transform(fit=False,labels=True,remove_info=True) # val data has the same bachelor info as the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### processing the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\AppData\\Local\\Temp/ipykernel_5768/1310676664.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['major_cluster']=major_clusters\n",
      "C:\\Users\\nguye\\AppData\\Local\\Temp/ipykernel_5768/1310676664.py:232: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['job_cluster']=jobs_clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing dates and text format \n",
      "\n",
      "completing position end dates...\n",
      "added 64030 values \n",
      "\n",
      "completing education end dates...\n",
      "added 5715 values \n",
      "\n",
      "completing education start dates...\n",
      "average length:  3.608725981355846 yrs\n",
      "added 879 values \n",
      "\n",
      "completing position end dates...\n",
      "average length:  3.962133030775858 yrs\n",
      "added 24 values \n",
      "\n",
      "completing education remaining dates...\n",
      "average start date:  2003.2742277481523 yrs\n",
      "average end date:  2006.882953729532 yrs\n",
      "added 18144 values \n",
      "\n",
      "completing position remaining dates...\n",
      "average start date:  2010.1799402438633 yrs\n",
      "average end date:  2014.1420732741742 yrs\n",
      "added 6696 values \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataTransformer = DataTransformer(df_positions_test,df_education_test,df_seniority_test)\n",
    "test_data =dataTransformer.transform(fit=False,labels=False,remove_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model\n",
    "\n",
    "#### After testing multiple models from sklearn, ensemble methods were the ones that worked the best\n",
    "\n",
    "#### We are going to investigate Neural networks later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting the data fo the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_to_matrix(df,feature_cols):\n",
    "        # convert to np array\n",
    "        X=np.asarray(df[feature_cols].values.tolist())\n",
    "        #reshape to matrix\n",
    "        X=np.reshape(X, (len(X), X.shape[1]*X.shape[2]))\n",
    "        if 'label' in df:\n",
    "            y=np.asarray(df[['label']]).reshape(-1)\n",
    "            return X,y\n",
    "        # else, testing data with no labels\n",
    "        return X\n",
    "    \n",
    "X_train,y_train=format_data_to_matrix(train_data,feature_cols)\n",
    "X_val,y_val=format_data_to_matrix(val_data,feature_cols)\n",
    "X_test=format_data_to_matrix(test_data,feature_cols) # no labels\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23751, 70) (23751,)\n",
      "(10180, 70) (10180,)\n",
      "(66069, 70)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)\n",
    "print(X_val.shape,y_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5; 1/10] START bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 1/5; 1/10] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.821 total time=   2.7s\n",
      "[CV 2/5; 1/10] START bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 2/5; 1/10] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.832 total time=   1.1s\n",
      "[CV 3/5; 1/10] START bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 3/5; 1/10] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.828 total time=   1.1s\n",
      "[CV 4/5; 1/10] START bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 4/5; 1/10] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.820 total time=   1.1s\n",
      "[CV 5/5; 1/10] START bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 5/5; 1/10] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.821 total time=   1.1s\n",
      "[CV 1/5; 2/10] START bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 1/5; 2/10] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.748 total time=  11.2s\n",
      "[CV 2/5; 2/10] START bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 2/5; 2/10] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.736 total time=  11.4s\n",
      "[CV 3/5; 2/10] START bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 3/5; 2/10] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.759 total time=  11.7s\n",
      "[CV 4/5; 2/10] START bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 4/5; 2/10] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.747 total time=  11.1s\n",
      "[CV 5/5; 2/10] START bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 5/5; 2/10] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.732 total time=  11.2s\n",
      "[CV 1/5; 3/10] START bootstrap=True, max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 1/5; 3/10] END bootstrap=True, max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.841 total time=   8.2s\n",
      "[CV 2/5; 3/10] START bootstrap=True, max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 2/5; 3/10] END bootstrap=True, max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.841 total time=   8.4s\n",
      "[CV 3/5; 3/10] START bootstrap=True, max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 3/5; 3/10] END bootstrap=True, max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.842 total time=   8.8s\n",
      "[CV 4/5; 3/10] START bootstrap=True, max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 4/5; 3/10] END bootstrap=True, max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.838 total time=   8.8s\n",
      "[CV 5/5; 3/10] START bootstrap=True, max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 5/5; 3/10] END bootstrap=True, max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.837 total time=   8.8s\n",
      "[CV 1/5; 4/10] START bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 1/5; 4/10] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.812 total time=   1.1s\n",
      "[CV 2/5; 4/10] START bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 2/5; 4/10] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.821 total time=   1.1s\n",
      "[CV 3/5; 4/10] START bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 3/5; 4/10] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.817 total time=   1.1s\n",
      "[CV 4/5; 4/10] START bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 4/5; 4/10] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.812 total time=   1.1s\n",
      "[CV 5/5; 4/10] START bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 5/5; 4/10] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.812 total time=   1.1s\n",
      "[CV 1/5; 5/10] START bootstrap=False, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 1/5; 5/10] END bootstrap=False, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.726 total time=  12.8s\n",
      "[CV 2/5; 5/10] START bootstrap=False, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 2/5; 5/10] END bootstrap=False, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.716 total time=  12.0s\n",
      "[CV 3/5; 5/10] START bootstrap=False, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 3/5; 5/10] END bootstrap=False, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.746 total time=  12.1s\n",
      "[CV 4/5; 5/10] START bootstrap=False, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 4/5; 5/10] END bootstrap=False, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.735 total time=  12.1s\n",
      "[CV 5/5; 5/10] START bootstrap=False, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 5/5; 5/10] END bootstrap=False, max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.719 total time=  12.2s\n",
      "[CV 1/5; 6/10] START bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 1/5; 6/10] END bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.824 total time=   1.2s\n",
      "[CV 2/5; 6/10] START bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/10] END bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.832 total time=   1.2s\n",
      "[CV 3/5; 6/10] START bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 3/5; 6/10] END bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.829 total time=   1.2s\n",
      "[CV 4/5; 6/10] START bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 4/5; 6/10] END bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.821 total time=   1.2s\n",
      "[CV 5/5; 6/10] START bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 5/5; 6/10] END bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.824 total time=   1.2s\n",
      "[CV 1/5; 7/10] START bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 1/5; 7/10] END bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.821 total time=   1.1s\n",
      "[CV 2/5; 7/10] START bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 2/5; 7/10] END bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.831 total time=   1.1s\n",
      "[CV 3/5; 7/10] START bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 3/5; 7/10] END bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.827 total time=   1.1s\n",
      "[CV 4/5; 7/10] START bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 4/5; 7/10] END bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.819 total time=   1.1s\n",
      "[CV 5/5; 7/10] START bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1\n",
      "[CV 5/5; 7/10] END bootstrap=True, max_depth=40, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200, n_jobs=-1;, score=0.822 total time=   1.1s\n",
      "[CV 1/5; 8/10] START bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 1/5; 8/10] END bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.837 total time=   5.1s\n",
      "[CV 2/5; 8/10] START bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 2/5; 8/10] END bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.838 total time=   5.0s\n",
      "[CV 3/5; 8/10] START bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 3/5; 8/10] END bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.838 total time=   5.1s\n",
      "[CV 4/5; 8/10] START bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 4/5; 8/10] END bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.834 total time=   5.0s\n",
      "[CV 5/5; 8/10] START bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 5/5; 8/10] END bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.833 total time=   5.1s\n",
      "[CV 1/5; 9/10] START bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 1/5; 9/10] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.725 total time=  12.2s\n",
      "[CV 2/5; 9/10] START bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 2/5; 9/10] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.721 total time=  12.0s\n",
      "[CV 3/5; 9/10] START bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 3/5; 9/10] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.748 total time=  12.2s\n",
      "[CV 4/5; 9/10] START bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 4/5; 9/10] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.735 total time=  11.7s\n",
      "[CV 5/5; 9/10] START bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200, n_jobs=-1\n",
      "[CV 5/5; 9/10] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200, n_jobs=-1;, score=0.716 total time=  11.7s\n",
      "[CV 1/5; 10/10] START bootstrap=False, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 1/5; 10/10] END bootstrap=False, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.824 total time=   1.9s\n",
      "[CV 2/5; 10/10] START bootstrap=False, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 2/5; 10/10] END bootstrap=False, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.837 total time=   1.9s\n",
      "[CV 3/5; 10/10] START bootstrap=False, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 3/5; 10/10] END bootstrap=False, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.831 total time=   1.9s\n",
      "[CV 4/5; 10/10] START bootstrap=False, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 4/5; 10/10] END bootstrap=False, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.823 total time=   1.9s\n",
      "[CV 5/5; 10/10] START bootstrap=False, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200, n_jobs=-1\n",
      "[CV 5/5; 10/10] END bootstrap=False, max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200, n_jobs=-1;, score=0.828 total time=   1.9s\n",
      "{'n_jobs': -1, 'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 40, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "#Params for search CV\n",
    "n_estimators = [200,400]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [20,40,60]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "njobs=[-1] # multiprocessing is a lot faster\n",
    "\n",
    "params = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "              'n_jobs' : njobs\n",
    "                }\n",
    "\n",
    "# Randomized search is faster \n",
    "search = RandomizedSearchCV(RandomForestRegressor(), param_grid, refit = True, verbose = 10) \n",
    "   \n",
    "search.fit(X_train, y_train) \n",
    " \n",
    "# print best parameter after tuning \n",
    "print(search.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score 0.8551950335051663\n",
      "explained_variance_score 0.8552052210438176\n",
      "validation mean abs error 2.083325215650968\n",
      "validation RMSE 3.9377920591012123\n"
     ]
    }
   ],
   "source": [
    "y_pred = search.predict(X_val)\n",
    "print('r2 score', search.score(X_val,y_val))\n",
    "print('explained_variance_score',explained_variance_score(y_val, y_pred))\n",
    "print('validation mean abs error',mean_absolute_error(y_val, y_pred))\n",
    "print('validation RMSE',mean_squared_error(y_val, y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler not necessary here but it make it easier to change model, when we have to use it\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('RandomForestRegressor', RandomForestRegressor(n_estimators=200,n_jobs=-1,min_samples_split=2,max_features='auto',min_samples_leaf=2,bootstrap=True,max_depth=60))])\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred = pipe.predict(X_val)\n",
    "print('r2 score', pipe.score(X_val,y_val))\n",
    "print('explained_variance_score',explained_variance_score(y_val, y_pred))\n",
    "print('validation mean abs error',mean_absolute_error(y_val, y_pred))\n",
    "print('validation RMSE',mean_squared_error(y_val, y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.asarray(test_data[feature_cols].values.tolist())\n",
    "# matrix dim for emsemble method\n",
    "X_test=np.reshape(X_test, (len(X_test), X_test.shape[1]*X_test.shape[2]))\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66069"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = y_pred\n",
    "test_data['true_or_predicted']=False\n",
    "\n",
    "train_data['true_or_predicted']=True\n",
    "val_data['true_or_predicted']=True\n",
    "\n",
    "df_all=pd.concat([train_data,val_data, test_data], axis=0)\n",
    "\n",
    "now = datetime.now()\n",
    "today=now.year+now.month/13\n",
    "\n",
    "df_all['age']=(today-df_all['label']).astype(int)\n",
    "df_all=df_all[['age','true_or_predicted']]\n",
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>true_or_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33931</th>\n",
       "      <td>++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>81</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33932</th>\n",
       "      <td>++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>58</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33933</th>\n",
       "      <td>++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33934</th>\n",
       "      <td>++7kB6m0hI1TgAPmyY1X6A5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33935</th>\n",
       "      <td>++9DtAOTiRRvECoMIpKbmg4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>35</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>zzZdW3VGODRxRl2025ZR2w5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>33</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>zzrNxfUzwZXNkSs15haLyA4ZM3TcQvn1bQ/jHgHWG0kf/b...</td>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>zzrbQXjc2yHwbWjtQ9F3mg5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>zzuZVPanBvW09lNk1C3h+Q5+2cvffV/mNepQVJd0smgtpB...</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66069 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 user_id  age  \\\n",
       "33931  ++5SW5MI5/h8X1hMA3QnmQ4ZM3TcQvn1bQ/jHgHWG0kf/b...   81   \n",
       "33932  ++5qk2+uEmkI/3Z4FrBwDw4ZM3TcQvn1bQ/jHgHWG0kf/b...   58   \n",
       "33933  ++6+hv3i5RAVsrWO8q5JEQ5+2cvffV/mNepQVJd0smgtpB...   27   \n",
       "33934  ++7kB6m0hI1TgAPmyY1X6A5+2cvffV/mNepQVJd0smgtpB...   40   \n",
       "33935  ++9DtAOTiRRvECoMIpKbmg4ZM3TcQvn1bQ/jHgHWG0kf/b...   35   \n",
       "...                                                  ...  ...   \n",
       "99995  zzZdW3VGODRxRl2025ZR2w5+2cvffV/mNepQVJd0smgtpB...   33   \n",
       "99996  zzrNxfUzwZXNkSs15haLyA4ZM3TcQvn1bQ/jHgHWG0kf/b...   55   \n",
       "99997  zzrbQXjc2yHwbWjtQ9F3mg5+2cvffV/mNepQVJd0smgtpB...   32   \n",
       "99998  zzuZVPanBvW09lNk1C3h+Q5+2cvffV/mNepQVJd0smgtpB...   31   \n",
       "99999  zzvZxBSf81furoFl3PcSuHAG1BvSkUYANepQVJd0smgtpB...   25   \n",
       "\n",
       "       true_or_predicted  \n",
       "33931              False  \n",
       "33932              False  \n",
       "33933              False  \n",
       "33934              False  \n",
       "33935              False  \n",
       "...                  ...  \n",
       "99995              False  \n",
       "99996              False  \n",
       "99997              False  \n",
       "99998              False  \n",
       "99999              False  \n",
       "\n",
       "[66069 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamere=df_all[~df_all['true_or_predicted']]\n",
    "tamere.reset_index(drop=True)\n",
    "tamere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      major   startdate     enddate\n",
      "99508    BS  1949-01-01  1953-01-01\n",
      "                                jobtitle   startdate enddate\n",
      "41525  former_owner_presently_consultant  1953-01-01     NaN\n",
      "                                jobtitle  seniority\n",
      "90141  former_owner_presently_consultant   7.064817\n",
      "=================\n",
      "                   major   startdate     enddate\n",
      "92083  BS in Electronics  1973-01-01  1978-01-01\n",
      "92505                NaN  1984-01-01  1987-01-01\n",
      "                                                 jobtitle   startdate  \\\n",
      "9781                    owner_|_computer_network_security  1993-06-01   \n",
      "106525  design_engineer_|_mechanical_industrial_engine...  1984-10-01   \n",
      "\n",
      "           enddate  \n",
      "9781           NaN  \n",
      "106525  1989-05-01  \n",
      "                                                 jobtitle  seniority\n",
      "71129   design_engineer_|_mechanical_industrial_engine...   3.331507\n",
      "222292                  owner_|_computer_network_security   7.334247\n",
      "=================\n",
      "                      major   startdate     enddate\n",
      "133238  Master Grande Ecole  2013-01-01  2016-01-01\n",
      "                                                 jobtitle   startdate  \\\n",
      "228241  digital_communication_press_relations_|_cosmetics  2016-07-01   \n",
      "228215             assistant_business_development_manager  2014-06-01   \n",
      "176460  trainee_digital_brand_commerce_crm_implementat...  2018-04-01   \n",
      "237149  trainee_digital_brand_commerce_digital_plannin...  2017-09-01   \n",
      "200727  sales_business_development_executive_|_events_...  2017-06-01   \n",
      "30076                     event_promoter_public_relations  2015-07-01   \n",
      "220136  trainee_digital_brand_commerce_membership_prog...  2018-09-01   \n",
      "23563          freelance_author_lucifer_manga_books_serie  2016-12-01   \n",
      "213101  digital_communication_social_medias_activation...  2016-11-01   \n",
      "\n",
      "           enddate  \n",
      "228241  2016-10-01  \n",
      "228215  2014-08-01  \n",
      "176460         NaN  \n",
      "237149  2018-03-01  \n",
      "200727  2017-08-01  \n",
      "30076   2015-08-01  \n",
      "220136         NaN  \n",
      "23563   2017-08-01  \n",
      "213101  2017-05-01  \n",
      "                                                 jobtitle  seniority\n",
      "399946  digital_communication_social_medias_activation...   4.307247\n",
      "220871                    event_promoter_public_relations   1.908356\n",
      "351436  trainee_digital_brand_commerce_membership_prog...   4.307247\n",
      "354892  trainee_digital_brand_commerce_digital_plannin...   4.307247\n",
      "111281  sales_business_development_executive_|_events_...   3.439384\n",
      "44321          freelance_author_lucifer_manga_books_serie   4.020771\n",
      "333670  digital_communication_press_relations_|_cosmetics   4.307247\n",
      "24521              assistant_business_development_manager   3.787412\n",
      "372408  trainee_digital_brand_commerce_crm_implementat...   4.307247\n",
      "=================\n",
      "                  major   startdate     enddate\n",
      "106134  Master's Degree  2008-01-01         NaN\n",
      "106774              NaN  2012-01-01  2013-01-01\n",
      "106190               BS  1996-01-01  1997-01-01\n",
      "                                                 jobtitle   startdate  \\\n",
      "311981                                           director  2012-08-01   \n",
      "324558                                  preschool_teacher  2011-06-01   \n",
      "280513            sr_research_associate_|_pharmaceuticals  1999-09-01   \n",
      "320187           validation_scientist_|_computer_software  2014-09-01   \n",
      "72818                           clinical_data_coordinator  2004-09-01   \n",
      "245906   computational_chemistry_intern_|_pharmaceuticals  1997-12-01   \n",
      "330491                                 substitute_teacher  2011-01-01   \n",
      "288582  validation_scientist_bristol_myers_squibb_co_|...  2005-05-01   \n",
      "351507           validation_scientist_|_computer_software  2008-03-01   \n",
      "\n",
      "           enddate  \n",
      "311981  2014-02-01  \n",
      "324558  2012-08-01  \n",
      "280513  2003-12-01  \n",
      "320187         NaN  \n",
      "72818   2005-05-01  \n",
      "245906  1998-12-01  \n",
      "330491  2011-05-01  \n",
      "288582  2007-09-01  \n",
      "351507  2012-12-01  \n",
      "                                                 jobtitle  seniority\n",
      "385257   computational_chemistry_intern_|_pharmaceuticals   4.142466\n",
      "343977  validation_scientist_bristol_myers_squibb_co_|...   2.750685\n",
      "20790                           clinical_data_coordinator   3.610914\n",
      "375097           validation_scientist_|_computer_software   4.713699\n",
      "231173            sr_research_associate_|_pharmaceuticals   5.046575\n",
      "248331                                           director   7.421538\n",
      "11467                                  substitute_teacher   2.039746\n",
      "343355                                  preschool_teacher   2.855363\n",
      "=================\n",
      "         major   startdate     enddate\n",
      "29395  B. Tech  2002-01-01  2006-01-01\n",
      "29488      MSc  2009-01-01  2010-01-01\n",
      "                                                jobtitle   startdate  \\\n",
      "149353                      business_development_manager  2007-01-01   \n",
      "163857  business_development_manager_|_computer_software  2007-07-01   \n",
      "136336          post_graduate_student_|_higher_education  2009-01-01   \n",
      "\n",
      "           enddate  \n",
      "149353  2007-07-01  \n",
      "163857  2008-12-01  \n",
      "136336         NaN  \n",
      "                                                jobtitle  seniority\n",
      "411244                      business_development_manager   5.325018\n",
      "251617  business_development_manager_|_computer_software   5.002740\n",
      "25815           post_graduate_student_|_higher_education   3.793151\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "for i in range (33931,33931+5):\n",
    "    user=tamere.loc[i,'user_id']\n",
    "    get_user_history(user)\n",
    "    print('=================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN and perception with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping the same split for consistency\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "\n",
    "X_train=np.reshape(X_train, (len(X_train),original_shape[1],original_shape[2]))\n",
    "X_train=np.swapaxes(X_train,1,2)\n",
    "X_val=scaler.transform(X_val) # no fit\n",
    "X_val=np.reshape(X_val, (len(X_val),original_shape[1],original_shape[2]))\n",
    "X_val=np.swapaxes(X_val,1,2)\n",
    "X_val.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_scaler=StandardScaler()\n",
    "y_train=label_scaler.fit_transform(y_train.reshape(-1, 1)).reshape(-1)\n",
    "y_val=label_scaler.transform(y_val.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential() \n",
    "model.add(layers.Input((original_shape[2],original_shape[1])))\n",
    "model.add(layers.LSTM(1024, return_sequences=True))\n",
    "model.add(layers.LSTM(256, input_shape=X.shape, return_sequences=False))\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "print(model.summary() )\n",
    "model.compile(loss='mean_squared_error',\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0003),\n",
    "              metrics=['accuracy']) \n",
    "history_LSTM = model.fit(X_train, y_train, validation_data=(X_val, y_val),batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model.predict(X_val)\n",
    "y_pred2 =label_scaler.inverse_transform(y_pred2.reshape(-1,1)).reshape(-1)\n",
    "y_true =label_scaler.inverse_transform(y_val.reshape(-1,1)).reshape(-1)\n",
    "accs=[abs(y_pred2[i]-y_true[i]) for i in range(len(y_true))]\n",
    "sum(accs)/len(accs)\n",
    "print('RMSE',mean_squared_error(y_true, y_pred,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.reshape(X_train, (len(X_train),original_shape[1]*original_shape[2]))\n",
    "X_val=np.reshape(X_val, (len(X_val),original_shape[1]*original_shape[2]))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "model_perceptron = tf.keras.Sequential() \n",
    "model_perceptron.add(layers.Input((original_shape[1]*original_shape[2])))\n",
    "model_perceptron.add(layers.Dense(30,activation='relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)))\n",
    "model_perceptron.add(layers.Dense(20,activation='relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)))\n",
    "model_perceptron.add(layers.Dense(10,activation='relu'))\n",
    "model_perceptron.add(layers.Dense(5,activation='relu'))\n",
    "model_perceptron.add(layers.Dense(5,activation='relu'))\n",
    "model_perceptron.add(layers.Dense(10,activation='relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)))\n",
    "model_perceptron.add(layers.Dense(1))\n",
    "\n",
    "print(model_perceptron.summary() )\n",
    "model_perceptron.compile(loss='mean_squared_error',\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001),\n",
    "              metrics=['accuracy']) \n",
    "history_perceptron = model_perceptron.fit(X_train, y_train, validation_data=(X_val, y_val),batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val =label_scaler.inverse_transform(y_val.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = model_perceptron.predict(X_val)\n",
    "print(y_pred3.shape)\n",
    "y_pred3 =label_scaler.inverse_transform(y_pred3.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "accs=[abs(y_pred3[i]-y_val[i]) for i in range(len(y_val))]\n",
    "print('avg abs err:',sum(accs)/len(accs))\n",
    "print('RMSE',mean_squared_error(y_val, y_pred3,squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on all the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaination end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majors_pipe=None\n",
    "jobs_pipe=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmean test, bachelor to be removed\n",
    "# data leakage but intuitively will improve the final accuracy\n",
    "\n",
    "texts=df_education_train['major'].tolist()\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text=[]\n",
    "for t in texts:\n",
    "    if (not pd.isnull(t)) and (not 'bachelor' in t.lower() or random.random()<0.33): # removing one third of bechelors to match the rest of the data set\n",
    "        processed_text.append(t)\n",
    "len(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 10\n",
    "majors_pipe = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 4))), ('majors_model', KMeans(n_clusters=number_of_clusters, init='k-means++', max_iter=50, n_init=2))])\n",
    "majors_pipe.fit(processed_text)\n",
    "\n",
    "\n",
    "\n",
    "order_centroids = majors_pipe['majors_model'].cluster_centers_.argsort()[:, ::-1]\n",
    "terms = majors_pipe['vectorizer'].get_feature_names_out()\n",
    "\n",
    "for i in range(0,number_of_clusters):\n",
    "    print(\"class %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cluster_edu(df):\n",
    "    majors=df['major'].astype(str).tolist()\n",
    "    major_clusters=majors_pipe.predict(majors)\n",
    "    df['major_cluster']=major_clusters\n",
    "add_cluster_edu(df_education_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cluster_edu(df_education_val)\n",
    "add_cluster_edu(df_education_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education_test.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_education_train[df_education_train['major_cluster']==3].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Usefull functions\n",
    "\n",
    "\n",
    "def to_lower_case(df,col):\n",
    "    df[col] =  df[col].str.lower()\n",
    "    return df \n",
    "def to_datetime(df,col):\n",
    "    df[col] =  pd.to_datetime(df[col], format='%Y-%m-%d')\n",
    "    df[col] =  df[col].dt.year+ df[col].dt.month/13\n",
    "    return df\n",
    "\n",
    "print('changing dates and text format \\n')\n",
    "\n",
    "df_education = df_education.pipe(to_lower_case, col='major').pipe(to_datetime,col='startdate').pipe(to_datetime,col='enddate')\n",
    "df_positions = df_positions.pipe(to_lower_case, col='jobtitle').pipe(to_datetime,col='startdate').pipe(to_datetime,col='enddate')\n",
    "df_seniority = df_seniority.pipe(to_lower_case, col='jobtitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values\n",
    "\n",
    "def fill_end_dates(df):\n",
    "    now = datetime.now()\n",
    "    today=now.year+now.month/13\n",
    "    #print('today',today)\n",
    "    before=df['enddate'].isna().sum()\n",
    "    df['enddate']=df.apply(lambda row: today if pd.isnull(row['enddate']) and (not pd.isnull(row['startdate'])) else row['enddate'],axis=1)\n",
    "    print('added',before-df['enddate'].isna().sum(),'values \\n')\n",
    "    return df\n",
    "\n",
    "print('completing position end dates...')\n",
    "\n",
    "df_positions=fill_end_dates(df_positions)\n",
    "print('completing education end dates...')\n",
    "df_education=fill_end_dates(df_education)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_startdate(df):\n",
    "    df_nona=df[['startdate','enddate']].dropna(how='all',inplace=False)\n",
    "    df_nona['length']=df_nona['enddate']-df_nona['startdate']\n",
    "    avg_length=df_nona[\"length\"].mean()\n",
    "    print('average length: ',avg_length,'yrs')\n",
    "    before=df['startdate'].isna().sum()\n",
    "    df['startdate'].fillna(df['enddate']-avg_length, inplace=True)\n",
    "    print('added',before-df['startdate'].isna().sum(),'values \\n')\n",
    "    return df\n",
    "print('completing education start dates...')\n",
    "df_education=fill_startdate(df_education)\n",
    "print('completing position end dates...')\n",
    "df_positions=fill_startdate(df_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dates_missingall(df):\n",
    "    df_nona=df[['startdate','enddate']].dropna(how='all',inplace=False)\n",
    "    #df_nona['length']=df_nona['enddate']-df_nona['startdate']\n",
    "    avg_start=df_nona[\"startdate\"].mean()\n",
    "    avg_end=df_nona[\"enddate\"].mean()\n",
    "    print('average start date: ',avg_start,'yrs')\n",
    "    print('average end date: ',avg_end,'yrs')\n",
    "    before=df['startdate'].isna().sum()\n",
    "    df['startdate'].fillna(avg_start, inplace=True)\n",
    "    df['enddate'].fillna(avg_end, inplace=True)\n",
    "    print('added',before-df['startdate'].isna().sum(),'values \\n')\n",
    "    return df  \n",
    "print('completing education remaining dates...')\n",
    "df_education=fill_dates_missingall(df_education)\n",
    "print('completing position remaining dates...')\n",
    "df_positions=fill_dates_missingall(df_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing education and positions\n",
    "df_positions['jobtitle'].fillna('positions', inplace=True)\n",
    "df_seniority['jobtitle'].fillna('positions', inplace=True)\n",
    "df_education['major'].fillna('education', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seniority['seniority'].fillna((df_seniority['seniority'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting by startdate for later:\n",
    "df_positions.sort_values(by=['startdate'],inplace=True)\n",
    "df_education.sort_values(by=['startdate'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_education.isna().sum())\n",
    "print(df_seniority.isna().sum())\n",
    "df_positions.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_positions.merge(df_seniority, how='right',left_on=['user_id','jobtitle'], right_on=['user_id','jobtitle']) # only position lacks users, hence right join\n",
    "df=df.groupby('user_id').agg(lambda x: list(x))\n",
    "df_education_temp=df_education.groupby('user_id').agg(lambda x: list(x))\n",
    "df=df.merge(df_education_temp, how='outer', left_on='user_id', right_on='user_id')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id ='++NWXZ8bW3I7VaXV77eiwA5+2cvffV/mNepQVJd0smgtpBr4MGMFJQ=='\n",
    "print(test_id)\n",
    "get_user_history(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count={'jobtitle': 0,'startdate_x':0,'enddate_x':0,'startdate_y':0,'startdate_y':0,'major':0,'seniority':0}\n",
    "mask=[]\n",
    "\n",
    "for rowIndex, row in df.iterrows(): \n",
    "    ismissing=False\n",
    "    for i, value in row.items():\n",
    "        for n in value:\n",
    "            if (pd.isnull(n)):\n",
    "                ismissing=True\n",
    "                missing_count[i]=missing_count[i]+1\n",
    "    if ismissing:\n",
    "        mask.append(True)\n",
    "    else:\n",
    "        mask.append(False)\n",
    "missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in mask if x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['startdate_x']=df.apply(lambda row: [max(row['enddate_y']) if pd.isnull(date) else date for date in row['startdate_x'] ] ,axis=1)\n",
    "now = datetime.now()\n",
    "today=now.year+now.month/13\n",
    "df['enddate_x']=df['enddate_x'].apply(lambda dates: [today if pd.isnull(date) else date for date in dates])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
